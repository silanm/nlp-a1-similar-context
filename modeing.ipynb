{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# CPU/GPU/MPS\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Download reuters corpus\n",
    "nltk.download(\"reuters\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "\n",
    "# Build reuters corpus\n",
    "def build_corpus(sample_size):\n",
    "    corpus = []\n",
    "    # Iterate through the first 'sample_size' files in the Reuters corpus\n",
    "    for file_id in reuters.fileids()[:sample_size]:\n",
    "        # Extract words from the file, convert to lowercase, and filter out non-alphabetic tokens\n",
    "        words = [word.lower() for word in reuters.words(file_id) if word.isalpha()]\n",
    "        # Append the processed words to the corpus\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# Vocabulary Building\n",
    "def build_vocab(corpus, min_freq=5):\n",
    "    # Flatten the corpus to get a list of all words\n",
    "    words = [word for sentence in corpus for word in sentence]\n",
    "    # Count the frequency of each word in the corpus\n",
    "    word_counts = Counter(words)\n",
    "    # Create a vocabulary list with words that have a frequency greater than or equal to 'min_freq'\n",
    "    vocab = [word for word, count in word_counts.items() if count >= min_freq]\n",
    "    # Add a special token for unknown words\n",
    "    vocab.append(\"<UNKNOWN>\")\n",
    "    # Create a mapping from words to their indices\n",
    "    word2index = {word: idx for idx, word in enumerate(vocab)}\n",
    "    # Ensure the unknown token is mapped to index 0\n",
    "    word2index[\"<UNKNOWN>\"] = 0\n",
    "    return vocab, word2index, word_counts\n",
    "\n",
    "\n",
    "# Generate Skip-grams\n",
    "def build_skipgrams(corpus, word2index, window_size):\n",
    "    skip_grams = []\n",
    "    # Iterate through each sentence in the corpus\n",
    "    for sentence in corpus:\n",
    "        # Iterate through each word in the sentence\n",
    "        for idx, word in enumerate(sentence):\n",
    "            # Get the index of the center word, default to <UNKNOWN> if not found\n",
    "            center = word2index.get(word, word2index[\"<UNKNOWN>\"])\n",
    "            # Define the context window around the center word\n",
    "            context_window = sentence[max(0, idx - window_size) : idx] + sentence[idx + 1 : idx + window_size + 1]\n",
    "            # Iterate through each context word in the context window\n",
    "            for context_word in context_window:\n",
    "                # Get the index of the context word, default to <UNKNOWN> if not found\n",
    "                context = word2index.get(context_word, word2index[\"<UNKNOWN>\"])\n",
    "                # Append the (center, context) pair to the skip_grams list\n",
    "                skip_grams.append((center, context))\n",
    "    return skip_grams\n",
    "\n",
    "\n",
    "def weighting_function(x_ij, x_max=100, alpha=0.75):\n",
    "    return (x_ij / x_max) ** alpha if x_ij < x_max else 1\n",
    "\n",
    "\n",
    "# Skipgram Model\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, center_words, context_words):\n",
    "        # Get the embeddings for the center and context words\n",
    "        center_embed = self.embedding_v(center_words)\n",
    "        context_embed = self.embedding_u(context_words)\n",
    "\n",
    "        # Compute the scores by taking the dot product of center and context embeddings\n",
    "        scores = torch.matmul(center_embed, context_embed.T)\n",
    "\n",
    "        # Apply log softmax to the scores to get the log probabilities\n",
    "        log_probs = torch.log_softmax(scores, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "# Skipgram Model with Negative Sampling\n",
    "class SkipgramNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, center_words, pos_context, neg_context):\n",
    "        # Get the embeddings for the center, positive context, and negative context words\n",
    "        center_embed = self.embedding_v(center_words)\n",
    "        pos_embed = self.embedding_u(pos_context)\n",
    "        neg_embed = self.embedding_u(neg_context)\n",
    "\n",
    "        # Compute the positive score by taking the dot product of center and positive context embeddings\n",
    "        pos_score = self.logsigmoid(torch.bmm(pos_embed.unsqueeze(1), center_embed.unsqueeze(2))).squeeze()\n",
    "\n",
    "        # Compute the negative score by taking the dot product of center and negative context embeddings\n",
    "        neg_score = self.logsigmoid(-torch.bmm(neg_embed, center_embed.unsqueeze(2))).squeeze()\n",
    "\n",
    "        # Calculate the loss as the negative sum of positive and negative scores\n",
    "        loss = -(pos_score.sum() + neg_score.sum())\n",
    "        return loss\n",
    "\n",
    "\n",
    "# GloVe Model\n",
    "class GloVe(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(GloVe, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size)\n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "    def forward(self, center_words, target_words, co_occurrences, weightings):\n",
    "        # Get the embeddings for the center and target words\n",
    "        center_embed = self.embedding_v(center_words)\n",
    "        target_embed = self.embedding_u(target_words)\n",
    "\n",
    "        # Get the biases for the center and target words\n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "\n",
    "        # Compute the inner product of the center and target embeddings\n",
    "        inner_product = (center_embed * target_embed).sum(dim=1)\n",
    "\n",
    "        # Calculate the loss using the weighting function and co-occurrences\n",
    "        loss = weightings * torch.pow(inner_product + center_bias + target_bias - co_occurrences, 2)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# Plotting Losses\n",
    "def plot_losses(losses, model_name):\n",
    "    plt.plot(losses)\n",
    "    plt.title(f\"Training Loss - {model_name}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Training Function for Skipgram Models\n",
    "def train_skipgram(model, skip_grams, epochs, batch_size, learning_rate, word2index, num_neg_samples=5):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []  # For plotting\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Shuffle the skip-grams for each epoch\n",
    "        random.shuffle(skip_grams)\n",
    "\n",
    "        for i in range(0, len(skip_grams), batch_size):\n",
    "            # Extract the current batch from the skip-grams\n",
    "            batch = skip_grams[i : i + batch_size]\n",
    "\n",
    "            # Unzip the batch into separate lists for center and context words\n",
    "            center_words, context_words = zip(*batch)\n",
    "\n",
    "            # Convert the lists to tensors and move them to CPU/GPU\n",
    "            center_words = torch.LongTensor(center_words).to(device)\n",
    "            context_words = torch.LongTensor(context_words).to(device)\n",
    "\n",
    "            # If the batch is smaller than the batch size, pad with zeros\n",
    "            if len(batch) < batch_size:\n",
    "                padding_size = batch_size - len(batch)\n",
    "                center_words = torch.cat([center_words, torch.zeros(padding_size, dtype=torch.long).to(device)])\n",
    "                context_words = torch.cat([context_words, torch.zeros(padding_size, dtype=torch.long).to(device)])\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            assert context_words.max().item() < len(word2index), \"Context word index out of bounds.\"\n",
    "            assert context_words.min().item() >= 0, \"Negative index found in context_words.\"\n",
    "\n",
    "            # Compute the loss based on the model type\n",
    "            if isinstance(model, Skipgram):\n",
    "                log_probs = model(center_words, context_words)\n",
    "                # print(f\"log_probs shape: {log_probs.shape}, context_words shape: {context_words.shape}\")\n",
    "\n",
    "                # Clip context_words to be the same size as log_probs\n",
    "                context_words = torch.clamp(context_words, max=log_probs.shape[1] - 1)\n",
    "\n",
    "                # For debugging on context_words\n",
    "                # assert context_words.max().item() < log_probs.shape[1], \"Index out of bounds in context_words\"\n",
    "\n",
    "                # Replace manual loss with NLLLoss to handle the shapes of log_probs and context_words\n",
    "                loss_fn = torch.nn.NLLLoss()\n",
    "                loss = loss_fn(log_probs, context_words)\n",
    "                # loss = -torch.mean(log_probs[range(batch_size), context_words])\n",
    "            else:\n",
    "                # neg_samples = torch.LongTensor([np.random.choice(len(word2index), num_neg_samples) for _ in range(batch_size)]).to(device)\n",
    "                neg_samples = (\n",
    "                    torch.from_numpy(np.random.choice(len(word2index), (batch_size, num_neg_samples))).long().to(device)\n",
    "                )  # Optimized version\n",
    "                loss = model(center_words, context_words, neg_samples)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(\n",
    "            f\"Model: {model.__class__.__name__:20s}| Epoch: {epoch + 1:-3d}/{epochs}  Loss: {total_loss:12.4f}  Time: {time.time() - start_time:6.2f}s\"\n",
    "        )\n",
    "        loss_history.append(total_loss)\n",
    "    plot_losses(loss_history, model.__class__.__name__)\n",
    "\n",
    "\n",
    "# Training Function for GloVe Model\n",
    "def train_glove_model(model, training_data, epochs, batch_size, learning_rate):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []  # For plotting\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Shuffle the training data for each epoch\n",
    "        random.shuffle(training_data)\n",
    "\n",
    "        for i in range(0, len(training_data), batch_size):\n",
    "            # Extract the current batch from the training data\n",
    "            batch = training_data[i : i + batch_size]\n",
    "\n",
    "            # Unzip the batch into separate lists for centers, contexts, co-occurrences, and weights\n",
    "            centers, contexts, coocs, weights = zip(*batch)\n",
    "\n",
    "            # Convert the lists to tensors and move them to CPU/GPU\n",
    "            centers = torch.LongTensor(centers).to(device)\n",
    "            contexts = torch.LongTensor(contexts).to(device)\n",
    "            coocs = torch.FloatTensor(coocs).to(device)\n",
    "            weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = model(centers, contexts, coocs, weights)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(\n",
    "            f\"Model: {model.__class__.__name__:20s}| Epoch: {epoch + 1:-3d}/{epochs}  Loss: {total_loss:12.4f}  Time: {time.time() - start_time:6.2f}s\"\n",
    "        )\n",
    "        loss_history.append(total_loss)\n",
    "    plot_losses(loss_history, model.__class__.__name__)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "# SAMPLE_SIZE = 1000  # Limit samples from the Reuters corpus\n",
    "SAMPLE_SIZE = len(reuters.fileids())  # Use the entire Reuters corpus\n",
    "WINDOW_SIZE = 2  # Context window size for skip-grams\n",
    "EMBEDDING_DIMENSION = 100  # Dimension of the embedding vectors\n",
    "TOTAL_EPOCHS = 50  # Number of epochs to train the models\n",
    "BATCH_SIZE = 256  # Batch size for training\n",
    "LEARNING_RATE = 0.001  # Learning rate for the optimizer\n",
    "\n",
    "# Build the corpus from the Reuters dataset\n",
    "corpus = build_corpus(SAMPLE_SIZE)\n",
    "\n",
    "# Build the vocabulary from the corpus with a minimum frequency threshold\n",
    "vocab, word2index, word_counts = build_vocab(corpus)\n",
    "\n",
    "# Generate skip-grams from the corpus using the built vocabulary\n",
    "skip_grams = build_skipgrams(corpus, word2index, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Models\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "model_skipgram = Skipgram(len(vocab), EMBEDDING_DIMENSION).to(device)\n",
    "train_skipgram(model_skipgram, skip_grams, TOTAL_EPOCHS, BATCH_SIZE, LEARNING_RATE, word2index)\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "model_skipgram_neg = SkipgramNegSampling(len(vocab), EMBEDDING_DIMENSION).to(device)\n",
    "train_skipgram(model_skipgram_neg, skip_grams, TOTAL_EPOCHS, BATCH_SIZE, LEARNING_RATE, word2index)\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "model_glove = GloVe(len(vocab), EMBEDDING_DIMENSION).to(device)\n",
    "co_occurrence_matrix = Counter(skip_grams)  # Prepare co-occurrence matrix\n",
    "training_data_glove = [(center, context, math.log(count + 1), weighting_function(count)) for (center, context), count in co_occurrence_matrix.items()]\n",
    "train_glove_model(model_glove, training_data_glove, TOTAL_EPOCHS, BATCH_SIZE, LEARNING_RATE)\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "\n",
    "# Save Models\n",
    "torch.save({\"model_state_dict\": model_skipgram.state_dict(), \"word2index\": word2index, \"vocab\": vocab}, \"skipgram_model.pth\")\n",
    "torch.save({\"model_state_dict\": model_skipgram_neg.state_dict(), \"word2index\": word2index, \"vocab\": vocab}, \"skipgram_neg_model.pth\")\n",
    "torch.save({\"model_state_dict\": model_glove.state_dict(), \"word2index\": word2index, \"vocab\": vocab}, \"glove_model.pth\")\n",
    "print(\"All models have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(filepath, model_class, vocab_size, embed_size):\n",
    "    checkpoint = torch.load(filepath, weights_only=False, map_location=device)\n",
    "    model = model_class(vocab_size, embed_size)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    return model, checkpoint[\"word2index\"], checkpoint[\"vocab\"]\n",
    "\n",
    "\n",
    "model_skipgram, word2index_skipgram, vocab_skipgram = load_model(\"skipgram_model.pth\", Skipgram, len(vocab), EMBEDDING_DIMENSION)\n",
    "model_skipgram_neg, word2index_neg, vocab_neg = load_model(\"skipgram_neg_model.pth\", SkipgramNegSampling, len(vocab), EMBEDDING_DIMENSION)\n",
    "model_glove, word2index_glove, vocab_glove = load_model(\"glove_model.pth\", GloVe, len(vocab), EMBEDDING_DIMENSION)\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_glove_gensim = KeyedVectors.load_word2vec_format(\"glove.6B.100d.txt\", binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Evaluate Analogies\n",
    "# TODO: Find Semantic Accuracy score using capital-common-countries section of word-test.v1.txt\n",
    "# TODO: Find Syntactic Accuracy score using gram7-past-tense section of word-test.v1.txt\n",
    "def evaluate_analogies(model, word2index, analogy_data, is_gensim=False):\n",
    "    semantic_correct, semantic_total = 0, 0\n",
    "    syntactic_correct, syntactic_total = 0, 0\n",
    "    section = None\n",
    "\n",
    "    for line in analogy_data:\n",
    "        if line.startswith(\":\"):\n",
    "            section = line.strip().lower()\n",
    "        else:\n",
    "            # Split the line into words and convert to lowercase\n",
    "            words = line.strip().lower().split()\n",
    "            # Check if all words are in the vocabulary\n",
    "            if all(word in word2index for word in words):\n",
    "                if is_gensim:\n",
    "                    predicted = model.most_similar(positive=[words[1], words[2]], negative=[words[0]], topn=1)[0][0]\n",
    "                else:\n",
    "                    # Get the indices of the words\n",
    "                    a, b, c, expected = [word2index[word] for word in words]\n",
    "                    # Calculate the vector for the predicted word\n",
    "                    vec = (\n",
    "                        model.embedding_v(torch.tensor([b])).detach()\n",
    "                        - model.embedding_v(torch.tensor([a])).detach()\n",
    "                        + model.embedding_v(torch.tensor([c])).detach()\n",
    "                    )\n",
    "                    # Compute similarities and find the word with the highest similarity\n",
    "                    similarities = torch.matmul(model.embedding_v.weight, vec.squeeze())\n",
    "                    predicted = torch.argmax(similarities).item()\n",
    "                # Check if the predicted word matches the expected word\n",
    "                if predicted == words[3]:\n",
    "                    if \"capital-common-countries\" in section:\n",
    "                        semantic_correct += 1\n",
    "                    elif \"gram7-past-tense\" in section:\n",
    "                        syntactic_correct += 1\n",
    "                # Update the total counts for semantic and syntactic sections\n",
    "                if \"capital-common-countries\" in section:\n",
    "                    semantic_total += 1\n",
    "                elif \"gram7-past-tense\" in section:\n",
    "                    syntactic_total += 1\n",
    "\n",
    "    # Calculate accuracy for semantic and syntactic sections\n",
    "    semantic_accuracy = semantic_correct / semantic_total if semantic_total else 0\n",
    "    syntactic_accuracy = syntactic_correct / syntactic_total if syntactic_total else 0\n",
    "    print(f\"    Semantic Accuracy (Capital-Common-Countries): {semantic_accuracy:.4f}\")\n",
    "    print(f\"    Syntactic Accuracy (Gram7-Past-Tense): {syntactic_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluate Similarity\n",
    "# TODO: Find the correlation between your models’ dot product and the provided similarity metrics (spearmanr). Output is MSE score.\n",
    "# TODO: Assess if your embeddings correlate with human judgment of word similarity against the wordsim_similarity_goldstandard.txt file.\n",
    "def evaluate_similarity(model, word2index, similarity_data, is_gensim=False):\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    for line in similarity_data:\n",
    "        # Split the line into words and the similarity score\n",
    "        word1, word2, score = line.strip().split()\n",
    "        # Check if both words are in the vocabulary\n",
    "        if word1 in word2index and word2 in word2index:\n",
    "            if is_gensim:\n",
    "                similarity = model.similarity(word1, word2)\n",
    "            else:\n",
    "                vec1 = model.embedding_v(torch.tensor([word2index[word1]])).detach()\n",
    "                vec2 = model.embedding_v(torch.tensor([word2index[word2]])).detach()\n",
    "                # Calculate the dot product similarity\n",
    "                similarity = torch.dot(vec1.squeeze(), vec2.squeeze()).item()\n",
    "            predictions.append(similarity)\n",
    "            ground_truth.append(float(score))\n",
    "    # Calculate Spearman correlation between predictions and ground truth\n",
    "    spearman_corr, _ = spearmanr(predictions, ground_truth)\n",
    "    # Calculate Mean Squared Error (MSE) between predictions and ground truth\n",
    "    mse_score = mean_squared_error(ground_truth, predictions)\n",
    "    print(f\"    Spearman Correlation: {spearman_corr:.4f}\")\n",
    "    print(f\"    Mean Squared Error (MSE): {mse_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analogy and similarity datasets\n",
    "with open(\"word-test.v1.txt\", \"r\") as file:\n",
    "    analogy_data = file.readlines()\n",
    "\n",
    "with open(\"wordsim_similarity_goldstandard.txt\", \"r\") as file:\n",
    "    similarity_data = file.readlines()\n",
    "\n",
    "# Run evaluation\n",
    "print(\"--- Skipgram Model ---\")\n",
    "evaluate_analogies(model_skipgram, word2index_skipgram, analogy_data)\n",
    "evaluate_similarity(model_skipgram, word2index_skipgram, similarity_data)\n",
    "\n",
    "print(\"\\n--- Skipgram Negative Sampling Model ---\")\n",
    "evaluate_analogies(model_skipgram_neg, word2index_neg, analogy_data)\n",
    "evaluate_similarity(model_skipgram_neg, word2index_neg, similarity_data)\n",
    "\n",
    "print(\"\\n--- GloVe PyTorch Model ---\")\n",
    "evaluate_analogies(model_glove, word2index_glove, analogy_data)\n",
    "evaluate_similarity(model_glove, word2index_glove, similarity_data)\n",
    "\n",
    "# This is taking too much time for Gensim to evaluate all the words\n",
    "\n",
    "# print(\"\\n--- GloVe Gensim Model ---\")\n",
    "# evaluate_analogies(model_glove_gensim, model_glove_gensim.key_to_index, analogy_data, is_gensim=True)\n",
    "# evaluate_similarity(model_glove_gensim, model_glove_gensim.key_to_index, similarity_data, is_gensim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Find Top 10 Most Similar Words Based on Dot Product\n",
    "def find_top_similar_words(model, word2index, index2word, input_word, top_n=10, is_gensim=False):\n",
    "    if is_gensim:\n",
    "        if input_word not in model:\n",
    "            print(f\"'{input_word}' not in vocabulary.\")\n",
    "            return\n",
    "        similar_words = model.most_similar(input_word, topn=top_n)\n",
    "        print(f\"Top {top_n} words similar to '{input_word}':\")\n",
    "        for word, similarity in similar_words:\n",
    "            print(f\"Model: {'Gensim':20s}  Word: {word:15s}  Similarity: {similarity:.4f}\")\n",
    "    else:\n",
    "        if input_word not in word2index:\n",
    "            print(f\"'{input_word}' not in vocabulary.\")\n",
    "            return\n",
    "        input_vec = model.embedding_v(torch.tensor([word2index[input_word]])).detach()\n",
    "        similarities = torch.matmul(model.embedding_v.weight, input_vec.squeeze())\n",
    "        probabilities = torch.softmax(similarities, dim=0)\n",
    "        top_indices = torch.topk(probabilities, top_n + 1).indices.tolist()[1:]\n",
    "        print(f\"Top {top_n} words similar to '{input_word}':\")\n",
    "        for idx in top_indices:\n",
    "            print(f\"Model: {model.__class__.__name__:20s}  Word: {index2word[idx]:15s}  Similarity: {similarities[idx]:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "\n",
    "input_word = random.choice(vocab)\n",
    "\n",
    "find_top_similar_words(model_skipgram, word2index_skipgram, vocab_skipgram, input_word)\n",
    "find_top_similar_words(model_skipgram_neg, word2index_neg, vocab_neg, input_word)\n",
    "find_top_similar_words(model_glove, word2index_glove, vocab_glove, input_word)\n",
    "find_top_similar_words(model_glove_gensim, model_glove_gensim.key_to_index, None, input_word, is_gensim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# Analogy Prediction Function\n",
    "def predict_analogy(model, word2index, word_a, word_b, word_c, is_gensim=False):\n",
    "    if is_gensim:\n",
    "        if all(word in model for word in [word_a, word_b, word_c]):\n",
    "            result = model.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)[0][0]\n",
    "            print(f\"{'Gensim':<20s}: '{word_a}' is to '{word_b}' as '{word_c}' is to '{result}'\")\n",
    "        else:\n",
    "            print(\"One or more input words are not in the vocabulary.\")\n",
    "    else:\n",
    "        if all(word in word2index for word in [word_a, word_b, word_c]):\n",
    "            a, b, c = [word2index[word] for word in [word_a, word_b, word_c]]\n",
    "            vec = (\n",
    "                model.embedding_v(torch.tensor([b])).detach()\n",
    "                - model.embedding_v(torch.tensor([a])).detach()\n",
    "                + model.embedding_v(torch.tensor([c])).detach()\n",
    "            )\n",
    "            similarities = torch.matmul(model.embedding_v.weight, vec.squeeze())\n",
    "            predicted_idx = torch.argmax(similarities).item()\n",
    "            predicted_word = vocab_glove[predicted_idx]\n",
    "            print(f\"{model.__class__.__name__:<20s}: '{word_a}' is to '{word_b}' as '{word_c}' is to '{predicted_word}'\")\n",
    "        else:\n",
    "            print(\"One or more input words are not in the vocabulary.\")\n",
    "\n",
    "\n",
    "first_word = random.choice(vocab)\n",
    "second_word = random.choice(vocab)  # The second word might not relate to the first word, it's random.\n",
    "third_word = random.choice(vocab)\n",
    "\n",
    "predict_analogy(model_skipgram, word2index_skipgram, first_word, second_word, third_word)\n",
    "predict_analogy(model_skipgram_neg, word2index_neg, first_word, second_word, third_word)\n",
    "predict_analogy(model_glove, word2index_glove, first_word, second_word, third_word)\n",
    "predict_analogy(model_glove_gensim, model_glove_gensim.key_to_index, first_word, second_word, third_word, is_gensim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Generate Sentence from a Single Input Word\n",
    "def generate_sentence(model, word2index, index2word, input_word, length=9, is_gensim=False):\n",
    "    if is_gensim:\n",
    "        if input_word not in model:\n",
    "            print(f\"'{input_word}' not in vocabulary.\")\n",
    "            return\n",
    "        sentence = [input_word]\n",
    "        current_word = input_word\n",
    "        previous_word = np.random.choice(list(model.key_to_index.keys()))\n",
    "        for _ in range(length):\n",
    "            next_word = model.most_similar(positive=[current_word], negative=[previous_word], topn=1)[0][0]\n",
    "            sentence.append(next_word)\n",
    "            previous_word = current_word\n",
    "            current_word = next_word\n",
    "        print(f\"{'Gensim':<20s}:\", \" \".join(sentence))\n",
    "    else:\n",
    "        if input_word not in word2index:\n",
    "            print(f\"'{input_word}' not in vocabulary.\")\n",
    "            return\n",
    "        sentence = [input_word]\n",
    "        current_idx = word2index[input_word]\n",
    "        for _ in range(length):\n",
    "            input_vec = model.embedding_v(torch.tensor([current_idx])).detach()\n",
    "            similarities = torch.matmul(model.embedding_v.weight, input_vec.squeeze())\n",
    "            next_idx = torch.argmax(similarities).item() + 1\n",
    "            next_word = index2word[next_idx]\n",
    "            sentence.append(next_word)\n",
    "            current_idx = next_idx\n",
    "        print(f\"{model.__class__.__name__:<20s}:\", \" \".join(sentence))\n",
    "\n",
    "\n",
    "input_word = random.choice(vocab)\n",
    "\n",
    "generate_sentence(model_skipgram, word2index_skipgram, vocab_skipgram, input_word)\n",
    "generate_sentence(model_skipgram_neg, word2index_neg, vocab_neg, input_word)\n",
    "generate_sentence(model_glove, word2index_glove, vocab_glove, input_word)\n",
    "generate_sentence(model_glove_gensim, model_glove_gensim.key_to_index, None, input_word, is_gensim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random_similarity(model, word2index, similarity_data, is_gensim=False):\n",
    "    # Extract word pairs that exist in both vocab and similarity dataset\n",
    "    valid_pairs = []\n",
    "    for line in similarity_data:\n",
    "        word1, word2, score = line.strip().split()\n",
    "        if word1 in word2index and word2 in word2index:\n",
    "            valid_pairs.append((word1, word2, float(score)))\n",
    "\n",
    "    # Randomly select 5 pairs\n",
    "    sampled_pairs = random.sample(valid_pairs, 5)\n",
    "\n",
    "    model_scores = []\n",
    "    human_scores = []\n",
    "    results = []\n",
    "\n",
    "    for word1, word2, human_score in sampled_pairs:\n",
    "        if is_gensim:\n",
    "            model_score = model.similarity(word1, word2)\n",
    "        else:\n",
    "            vec1 = model.embedding_v(torch.tensor([word2index[word1]])).detach()\n",
    "            vec2 = model.embedding_v(torch.tensor([word2index[word2]])).detach()\n",
    "            model_score = torch.dot(vec1.squeeze(), vec2.squeeze()).item()  # Dot product to get similarity\n",
    "\n",
    "        model_scores.append(model_score)\n",
    "        human_scores.append(human_score)\n",
    "        results.append((word1, word2, model_score, human_score))\n",
    "\n",
    "    spearman_corr, _ = spearmanr(model_scores, human_scores)\n",
    "\n",
    "    print(f\"{model.__class__.__name__ if not is_gensim else 'Gensim'}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Word1':<20s}{'Word2':<20s}{'Model Score':<15s}{'Human Score':<15s}\")\n",
    "    print(\"-\" * 70)\n",
    "    for word1, word2, model_score, human_score in results:\n",
    "        print(f\"{word1:<20s}{word2:<20s}{model_score:<15.4f}{human_score:<15.4f}\")\n",
    "    print(f\"{'*' * 70}\\nSpearman Correlation: {spearman_corr:.4f}\\n{'*' * 70}\\n\")\n",
    "\n",
    "\n",
    "evaluate_random_similarity(model_skipgram, word2index_skipgram, similarity_data)\n",
    "evaluate_random_similarity(model_skipgram_neg, word2index_neg, similarity_data)\n",
    "evaluate_random_similarity(model_glove, word2index_glove, similarity_data)\n",
    "evaluate_random_similarity(model_glove_gensim, model_glove_gensim.key_to_index, similarity_data, is_gensim=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
