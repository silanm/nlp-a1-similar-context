{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/silanm/Developer/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/silanm/Developer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "# Build Reuters Corpus\n",
    "def build_corpus(sample_size):\n",
    "    corpus = []\n",
    "    # Iterate through the first 'sample_size' files in the Reuters corpus\n",
    "    for file_id in reuters.fileids()[:sample_size]:\n",
    "        # Extract words from the file, convert to lowercase, and filter out non-alphabetic tokens\n",
    "        words = [word.lower() for word in reuters.words(file_id) if word.isalpha()]\n",
    "        # Append the processed words to the corpus\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# Vocabulary Building\n",
    "def build_vocab(corpus, min_freq=5):\n",
    "    # Flatten the corpus to get a list of all words\n",
    "    words = [word for sentence in corpus for word in sentence]\n",
    "\n",
    "    # Count the frequency of each word in the corpus\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Create a vocabulary list with words that have a frequency greater than or equal to 'min_freq'\n",
    "    vocab = [word for word, count in word_counts.items() if count >= min_freq]\n",
    "\n",
    "    # Add a special token for unknown words\n",
    "    vocab.append(\"<UNKNOWN>\")\n",
    "\n",
    "    # Create a mapping from words to their indices\n",
    "    word2index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    # Ensure the unknown token is mapped to index 0\n",
    "    word2index[\"<UNKNOWN>\"] = 0\n",
    "\n",
    "    return vocab, word2index, word_counts\n",
    "\n",
    "\n",
    "# Generate Skip-grams\n",
    "def build_skipgrams(corpus, word2index, window_size):\n",
    "    skip_grams = []\n",
    "    # Iterate through each sentence in the corpus\n",
    "    for sentence in corpus:\n",
    "        # Iterate through each word in the sentence\n",
    "        for idx, word in enumerate(sentence):\n",
    "            # Get the index of the center word, default to <UNKNOWN> if not found\n",
    "            center = word2index.get(word, word2index[\"<UNKNOWN>\"])\n",
    "            # Define the context window around the center word\n",
    "            context_window = sentence[max(0, idx - window_size) : idx] + sentence[idx + 1 : idx + window_size + 1]\n",
    "            # Iterate through each context word in the context window\n",
    "            for context_word in context_window:\n",
    "                # Get the index of the context word, default to <UNKNOWN> if not found\n",
    "                context = word2index.get(context_word, word2index[\"<UNKNOWN>\"])\n",
    "                # Append the (center, context) pair to the skip_grams list\n",
    "                skip_grams.append((center, context))\n",
    "    return skip_grams\n",
    "\n",
    "\n",
    "def weighting_function(x_ij, x_max=100, alpha=0.75):\n",
    "    return (x_ij / x_max) ** alpha if x_ij < x_max else 1\n",
    "\n",
    "\n",
    "# Skipgram Model\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, center_words, context_words):\n",
    "        # Get the embeddings for the center and context words\n",
    "        center_embed = self.embedding_v(center_words)\n",
    "        context_embed = self.embedding_u(context_words)\n",
    "\n",
    "        # Compute the scores by taking the dot product of center and context embeddings\n",
    "        scores = torch.matmul(center_embed, context_embed.T)\n",
    "\n",
    "        # Apply log softmax to the scores to get the log probabilities\n",
    "        log_probs = torch.log_softmax(scores, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "# Skipgram Model with Negative Sampling\n",
    "class SkipgramNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, center_words, pos_context, neg_context):\n",
    "        # Get the embeddings for the center, positive context, and negative context words\n",
    "        center_embed = self.embedding_v(center_words)\n",
    "        pos_embed = self.embedding_u(pos_context)\n",
    "        neg_embed = self.embedding_u(neg_context)\n",
    "\n",
    "        # Compute the positive score by taking the dot product of center and positive context embeddings\n",
    "        pos_score = self.logsigmoid(torch.bmm(pos_embed.unsqueeze(1), center_embed.unsqueeze(2))).squeeze()\n",
    "\n",
    "        # Compute the negative score by taking the dot product of center and negative context embeddings\n",
    "        neg_score = self.logsigmoid(-torch.bmm(neg_embed, center_embed.unsqueeze(2))).squeeze()\n",
    "\n",
    "        # Calculate the loss as the negative sum of positive and negative scores\n",
    "        loss = -(pos_score.sum() + neg_score.sum())\n",
    "        return loss\n",
    "\n",
    "\n",
    "# GloVe Model\n",
    "class GloVe(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(GloVe, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size)\n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "    def forward(self, center_words, target_words, co_occurrences, weightings):\n",
    "        # Get the embeddings for the center and target words\n",
    "        center_embed = self.embedding_v(center_words)\n",
    "        target_embed = self.embedding_u(target_words)\n",
    "\n",
    "        # Get the biases for the center and target words\n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "\n",
    "        # Compute the inner product of the center and target embeddings\n",
    "        inner_product = (center_embed * target_embed).sum(dim=1)\n",
    "\n",
    "        # Calculate the loss using the weighting function and co-occurrences\n",
    "        loss = weightings * torch.pow(inner_product + center_bias + target_bias - co_occurrences, 2)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# Plotting Loss Function\n",
    "def plot_losses(losses, model_name):\n",
    "    plt.plot(losses)\n",
    "    plt.title(f\"Training Loss - {model_name}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Training Function for Skipgram Models\n",
    "def train_skipgram(model, skip_grams, epochs, batch_size, learning_rate, word2index, num_neg_samples=5):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Shuffle the skip-grams for each epoch\n",
    "        random.shuffle(skip_grams)\n",
    "\n",
    "        for i in range(0, len(skip_grams), batch_size):\n",
    "            # Extract the current batch from the skip-grams\n",
    "            batch = skip_grams[i : i + batch_size]\n",
    "\n",
    "            # Unzip the batch into separate lists for center and context words\n",
    "            center_words, context_words = zip(*batch)\n",
    "\n",
    "            # Convert the lists to tensors and move them to CPU/GPU\n",
    "            center_words = torch.LongTensor(center_words).to(device)\n",
    "            context_words = torch.LongTensor(context_words).to(device)\n",
    "\n",
    "            # If the batch is smaller than the batch size, pad with zeros\n",
    "            if len(batch) < batch_size:\n",
    "                padding_size = batch_size - len(batch)\n",
    "                center_words = torch.cat([center_words, torch.zeros(padding_size, dtype=torch.long).to(device)])\n",
    "                context_words = torch.cat([context_words, torch.zeros(padding_size, dtype=torch.long).to(device)])\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            assert context_words.max().item() < len(word2index), \"Context word index out of bounds.\"\n",
    "            assert context_words.min().item() >= 0, \"Negative index found in context_words.\"\n",
    "\n",
    "            # Compute the loss based on the model type\n",
    "            if isinstance(model, Skipgram):\n",
    "                log_probs = model(center_words, context_words)\n",
    "                # print(f\"log_probs shape: {log_probs.shape}, context_words shape: {context_words.shape}\")\n",
    "\n",
    "                # Clip context_words to avoid index errors\n",
    "                context_words = torch.clamp(context_words, max=log_probs.shape[1] - 1)\n",
    "                assert context_words.max().item() < log_probs.shape[1], \"Index out of bounds in context_words\"\n",
    "\n",
    "                # Replace manual loss with NLLLoss\n",
    "                loss_fn = torch.nn.NLLLoss()\n",
    "                loss = loss_fn(log_probs, context_words)\n",
    "                # loss = -torch.mean(log_probs[range(batch_size), context_words])\n",
    "            else:\n",
    "                # neg_samples = torch.LongTensor([np.random.choice(len(word2index), num_neg_samples) for _ in range(batch_size)]).to(device)\n",
    "                neg_samples = torch.from_numpy(np.random.choice(len(word2index), (batch_size, num_neg_samples))).long().to(device)\n",
    "                loss = model(center_words, context_words, neg_samples)\n",
    "\n",
    "            # Backpropagate the loss and update the model parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(\n",
    "            f\"Model: {model.__class__.__name__:20s}| Epoch: {epoch + 1:-3d}/{epochs}  Loss: {total_loss:12.4f}  Time: {time.time() - start_time:6.2f}s\"\n",
    "        )\n",
    "        loss_history.append(total_loss)\n",
    "    plot_losses(loss_history, model.__class__.__name__)\n",
    "\n",
    "\n",
    "def train_glove_model(model, training_data, epochs, batch_size, learning_rate):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Shuffle the training data for each epoch\n",
    "        random.shuffle(training_data)\n",
    "\n",
    "        for i in range(0, len(training_data), batch_size):\n",
    "            # Extract the current batch from the training data\n",
    "            batch = training_data[i : i + batch_size]\n",
    "\n",
    "            # Unzip the batch into separate lists for centers, contexts, co-occurrences, and weights\n",
    "            centers, contexts, coocs, weights = zip(*batch)\n",
    "\n",
    "            # Convert the lists to tensors and move them to CPU/GPU\n",
    "            centers = torch.LongTensor(centers).to(device)\n",
    "            contexts = torch.LongTensor(contexts).to(device)\n",
    "            coocs = torch.FloatTensor(coocs).to(device)\n",
    "            weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = model(centers, contexts, coocs, weights)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(\n",
    "            f\"Model: {model.__class__.__name__:20s}| Epoch: {epoch + 1:-3d}/{epochs}  Loss: {total_loss:12.4f}  Time: {time.time() - start_time:6.2f}s\"\n",
    "        )\n",
    "        loss_history.append(total_loss)\n",
    "    plot_losses(loss_history, model.__class__.__name__)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "# SAMPLE_SIZE = 1000  # Number of documents to sample from the Reuters corpus\n",
    "SAMPLE_SIZE = len(reuters.fileids())\n",
    "WINDOW_SIZE = 2  # Context window size for skip-grams\n",
    "EMBEDDING_DIMENSION = 100  # Dimension of the embedding vectors\n",
    "TOTAL_EPOCHS = 50  # Number of epochs to train the models\n",
    "BATCH_SIZE = 256  # Batch size for training\n",
    "LEARNING_RATE = 0.001  # Learning rate for the optimizer\n",
    "\n",
    "# Build the corpus from the Reuters dataset\n",
    "corpus = build_corpus(SAMPLE_SIZE)\n",
    "\n",
    "# Build the vocabulary from the corpus with a minimum frequency threshold\n",
    "vocab, word2index, word_counts = build_vocab(corpus)\n",
    "\n",
    "# Generate skip-grams from the corpus using the built vocabulary\n",
    "skip_grams = build_skipgrams(corpus, word2index, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Models\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "model_skipgram = Skipgram(len(vocab), EMBEDDING_DIMENSION).to(device)\n",
    "train_skipgram(model_skipgram, skip_grams, TOTAL_EPOCHS, BATCH_SIZE, LEARNING_RATE, word2index)\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "model_skipgram_neg = SkipgramNegSampling(len(vocab), EMBEDDING_DIMENSION).to(device)\n",
    "train_skipgram(model_skipgram_neg, skip_grams, TOTAL_EPOCHS, BATCH_SIZE, LEARNING_RATE, word2index)\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "model_glove = GloVe(len(vocab), EMBEDDING_DIMENSION).to(device)\n",
    "co_occurrence_matrix = Counter(skip_grams)  # Prepare co-occurrence matrix\n",
    "training_data_glove = [(center, context, math.log(count + 1), weighting_function(count)) for (center, context), count in co_occurrence_matrix.items()]\n",
    "train_glove_model(model_glove, training_data_glove, TOTAL_EPOCHS, BATCH_SIZE, LEARNING_RATE)\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "\n",
    "# Save Models\n",
    "torch.save({\"model_state_dict\": model_skipgram.state_dict(), \"word2index\": word2index, \"vocab\": vocab}, \"skipgram_model.pth\")\n",
    "torch.save({\"model_state_dict\": model_skipgram_neg.state_dict(), \"word2index\": word2index, \"vocab\": vocab}, \"skipgram_neg_model.pth\")\n",
    "torch.save({\"model_state_dict\": model_glove.state_dict(), \"word2index\": word2index, \"vocab\": vocab}, \"glove_model.pth\")\n",
    "print(\"All models have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(filepath, model_class, vocab_size, embed_size):\n",
    "    checkpoint = torch.load(filepath, weights_only=False, map_location=device)\n",
    "    model = model_class(vocab_size, embed_size)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    return model, checkpoint[\"word2index\"], checkpoint[\"vocab\"]\n",
    "\n",
    "\n",
    "model_skipgram, word2index_skipgram, vocab_skipgram = load_model(\"skipgram_model.pth\", Skipgram, len(vocab), EMBEDDING_DIMENSION)\n",
    "model_skipgram_neg, word2index_neg, vocab_neg = load_model(\"skipgram_neg_model.pth\", SkipgramNegSampling, len(vocab), EMBEDDING_DIMENSION)\n",
    "model_glove, word2index_glove, vocab_glove = load_model(\"glove_model.pth\", GloVe, len(vocab), EMBEDDING_DIMENSION)\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_glove_gensim = KeyedVectors.load_word2vec_format(\"glove.6B.100d.txt\", binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Evaluate Analogies\n",
    "# TODO: Find Semantic Accuracy score using capital-common-countries section of word-test.v1.txt\n",
    "# TODO: Find Syntactic Accuracy score using gram7-past-tense section of word-test.v1.txt\n",
    "def evaluate_analogies(model, word2index, analogy_data, is_gensim=False):\n",
    "    semantic_correct, semantic_total = 0, 0\n",
    "    syntactic_correct, syntactic_total = 0, 0\n",
    "    section = None\n",
    "\n",
    "    for line in analogy_data:\n",
    "        if line.startswith(\":\"):\n",
    "            section = line.strip().lower()\n",
    "        else:\n",
    "            # Split the line into words and convert to lowercase\n",
    "            words = line.strip().lower().split()\n",
    "            # Check if all words are in the vocabulary\n",
    "            if all(word in word2index for word in words):\n",
    "                if is_gensim:\n",
    "                    predicted = model.most_similar(positive=[words[1], words[2]], negative=[words[0]], topn=1)[0][0]\n",
    "                else:\n",
    "                    # Get the indices of the words\n",
    "                    a, b, c, expected = [word2index[word] for word in words]\n",
    "                    # Calculate the vector for the predicted word\n",
    "                    vec = (\n",
    "                        model.embedding_v(torch.tensor([b])).detach()\n",
    "                        - model.embedding_v(torch.tensor([a])).detach()\n",
    "                        + model.embedding_v(torch.tensor([c])).detach()\n",
    "                    )\n",
    "                    # Compute similarities and find the word with the highest similarity\n",
    "                    similarities = torch.matmul(model.embedding_v.weight, vec.squeeze())\n",
    "                    predicted = torch.argmax(similarities).item()\n",
    "                # Check if the predicted word matches the expected word\n",
    "                if predicted == words[3]:\n",
    "                    if \"capital-common-countries\" in section:\n",
    "                        semantic_correct += 1\n",
    "                    elif \"gram7-past-tense\" in section:\n",
    "                        syntactic_correct += 1\n",
    "                # Update the total counts for semantic and syntactic sections\n",
    "                if \"capital-common-countries\" in section:\n",
    "                    semantic_total += 1\n",
    "                elif \"gram7-past-tense\" in section:\n",
    "                    syntactic_total += 1\n",
    "\n",
    "    # Calculate accuracy for semantic and syntactic sections\n",
    "    semantic_accuracy = semantic_correct / semantic_total if semantic_total else 0\n",
    "    syntactic_accuracy = syntactic_correct / syntactic_total if syntactic_total else 0\n",
    "    print(f\"    Semantic Accuracy (Capital-Common-Countries): {semantic_accuracy:.4f}\")\n",
    "    print(f\"    Syntactic Accuracy (Gram7-Past-Tense): {syntactic_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluate Similarity\n",
    "# TODO: Find the correlation between your models’ dot product and the provided similarity metrics (spearmanr). Output is MSE score.\n",
    "# TODO: Assess if your embeddings correlate with human judgment of word similarity against the wordsim_similarity_goldstandard.txt file.\n",
    "def evaluate_similarity(model, word2index, similarity_data, is_gensim=False):\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    for line in similarity_data:\n",
    "        # Split the line into words and the similarity score\n",
    "        word1, word2, score = line.strip().split()\n",
    "        # Check if both words are in the vocabulary\n",
    "        if word1 in word2index and word2 in word2index:\n",
    "            if is_gensim:\n",
    "                similarity = model.similarity(word1, word2)\n",
    "            else:\n",
    "                vec1 = model.embedding_v(torch.tensor([word2index[word1]])).detach()\n",
    "                vec2 = model.embedding_v(torch.tensor([word2index[word2]])).detach()\n",
    "                # Calculate the dot product similarity\n",
    "                similarity = torch.dot(vec1.squeeze(), vec2.squeeze()).item()\n",
    "            predictions.append(similarity)\n",
    "            ground_truth.append(float(score))\n",
    "    # Calculate Spearman correlation between predictions and ground truth\n",
    "    spearman_corr, _ = spearmanr(predictions, ground_truth)\n",
    "    # Calculate Mean Squared Error (MSE) between predictions and ground truth\n",
    "    mse_score = mean_squared_error(ground_truth, predictions)\n",
    "    print(f\"    Spearman Correlation: {spearman_corr:.4f}\")\n",
    "    print(f\"    Mean Squared Error (MSE): {mse_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Skipgram Model ---\n",
      "    Semantic Accuracy (Capital-Common-Countries): 0.0000\n",
      "    Syntactic Accuracy (Gram7-Past-Tense): 0.0000\n",
      "    Spearman Correlation: 0.3420\n",
      "    Mean Squared Error (MSE): 24.1058\n",
      "\n",
      "--- Skipgram Negative Sampling Model ---\n",
      "    Semantic Accuracy (Capital-Common-Countries): 0.0000\n",
      "    Syntactic Accuracy (Gram7-Past-Tense): 0.0000\n",
      "    Spearman Correlation: 0.3238\n",
      "    Mean Squared Error (MSE): 165.4730\n",
      "\n",
      "--- GloVe PyTorch Model ---\n",
      "    Semantic Accuracy (Capital-Common-Countries): 0.0000\n",
      "    Syntactic Accuracy (Gram7-Past-Tense): 0.0000\n",
      "    Spearman Correlation: 0.2178\n",
      "    Mean Squared Error (MSE): 30.8277\n"
     ]
    }
   ],
   "source": [
    "# Load analogy and similarity datasets\n",
    "with open(\"word-test.v1.txt\", \"r\") as file:\n",
    "    analogy_data = file.readlines()\n",
    "\n",
    "with open(\"wordsim_similarity_goldstandard.txt\", \"r\") as file:\n",
    "    similarity_data = file.readlines()\n",
    "\n",
    "# Run evaluation\n",
    "print(\"--- Skipgram Model ---\")\n",
    "evaluate_analogies(model_skipgram, word2index_skipgram, analogy_data)\n",
    "evaluate_similarity(model_skipgram, word2index_skipgram, similarity_data)\n",
    "\n",
    "print(\"\\n--- Skipgram Negative Sampling Model ---\")\n",
    "evaluate_analogies(model_skipgram_neg, word2index_neg, analogy_data)\n",
    "evaluate_similarity(model_skipgram_neg, word2index_neg, similarity_data)\n",
    "\n",
    "print(\"\\n--- GloVe PyTorch Model ---\")\n",
    "evaluate_analogies(model_glove, word2index_glove, analogy_data)\n",
    "evaluate_similarity(model_glove, word2index_glove, similarity_data)\n",
    "\n",
    "# print(\"\\n--- GloVe Gensim Model ---\")\n",
    "# evaluate_analogies(model_glove_gensim, model_glove_gensim.key_to_index, analogy_data, is_gensim=True)\n",
    "# evaluate_similarity(model_glove_gensim, model_glove_gensim.key_to_index, similarity_data, is_gensim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words similar to 'elder':\n",
      "Model: Skipgram              Word: hanna            Similarity: 13.0549\n",
      "Model: Skipgram              Word: copperbelt       Similarity: 12.6026\n",
      "Model: Skipgram              Word: dravo            Similarity: 12.2931\n",
      "Model: Skipgram              Word: chaco            Similarity: 11.7559\n",
      "Model: Skipgram              Word: gkn              Similarity: 11.6995\n",
      "Model: Skipgram              Word: participations   Similarity: 11.5282\n",
      "Model: Skipgram              Word: intersections    Similarity: 11.3954\n",
      "Model: Skipgram              Word: escalating       Similarity: 11.3659\n",
      "Model: Skipgram              Word: monsod           Similarity: 11.3020\n",
      "Model: Skipgram              Word: ky               Similarity: 11.2483\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 words similar to 'elder':\n",
      "Model: SkipgramNegSampling   Word: beerman          Similarity: 118.4066\n",
      "Model: SkipgramNegSampling   Word: u                Similarity: 3.2509\n",
      "Model: SkipgramNegSampling   Word: fear             Similarity: -17.3552\n",
      "Model: SkipgramNegSampling   Word: s                Similarity: 4.0982\n",
      "Model: SkipgramNegSampling   Word: trade            Similarity: -7.3396\n",
      "Model: SkipgramNegSampling   Word: from             Similarity: 6.8110\n",
      "Model: SkipgramNegSampling   Word: friction         Similarity: -16.4545\n",
      "Model: SkipgramNegSampling   Word: mounting         Similarity: -35.2108\n",
      "Model: SkipgramNegSampling   Word: asian            Similarity: 4.7019\n",
      "Model: SkipgramNegSampling   Word: exporters        Similarity: -11.4938\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 words similar to 'elder':\n",
      "Model: GloVe                 Word: cms              Similarity: 30.9220\n",
      "Model: GloVe                 Word: taxable          Similarity: 30.8529\n",
      "Model: GloVe                 Word: blds             Similarity: 29.3260\n",
      "Model: GloVe                 Word: norrell          Similarity: 29.1522\n",
      "Model: GloVe                 Word: conger           Similarity: 28.8582\n",
      "Model: GloVe                 Word: editorial        Similarity: 27.3010\n",
      "Model: GloVe                 Word: reiners          Similarity: 26.4428\n",
      "Model: GloVe                 Word: societies        Similarity: 26.3717\n",
      "Model: GloVe                 Word: lacklustre       Similarity: 25.6563\n",
      "Model: GloVe                 Word: smoky            Similarity: 24.9528\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 words similar to 'elder':\n",
      "Model: Gensim                Word: brother          Similarity: 0.7588\n",
      "Model: Gensim                Word: son              Similarity: 0.7580\n",
      "Model: Gensim                Word: eldest           Similarity: 0.7420\n",
      "Model: Gensim                Word: nephew           Similarity: 0.7354\n",
      "Model: Gensim                Word: father           Similarity: 0.7340\n",
      "Model: Gensim                Word: cousin           Similarity: 0.7324\n",
      "Model: Gensim                Word: grandson         Similarity: 0.7208\n",
      "Model: Gensim                Word: sons             Similarity: 0.7117\n",
      "Model: Gensim                Word: grandfather      Similarity: 0.7099\n",
      "Model: Gensim                Word: younger          Similarity: 0.6986\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Find Top 10 Most Similar Words Based on Dot Product\n",
    "def find_top_similar_words(model, word2index, index2word, input_word, top_n=10, is_gensim=False):\n",
    "    if is_gensim:\n",
    "        if input_word not in model:\n",
    "            print(f\"'{input_word}' not in vocabulary.\")\n",
    "            return\n",
    "        similar_words = model.most_similar(input_word, topn=top_n)\n",
    "        print(f\"Top {top_n} words similar to '{input_word}':\")\n",
    "        for word, similarity in similar_words:\n",
    "            print(f\"Model: {'Gensim':20s}  Word: {word:15s}  Similarity: {similarity:.4f}\")\n",
    "    else:\n",
    "        if input_word not in word2index:\n",
    "            print(f\"'{input_word}' not in vocabulary.\")\n",
    "            return\n",
    "        input_vec = model.embedding_v(torch.tensor([word2index[input_word]])).detach()\n",
    "        similarities = torch.matmul(model.embedding_v.weight, input_vec.squeeze())\n",
    "        probabilities = torch.softmax(similarities, dim=0)\n",
    "        top_indices = torch.topk(probabilities, top_n + 1).indices.tolist()[1:]\n",
    "        print(f\"Top {top_n} words similar to '{input_word}':\")\n",
    "        for idx in top_indices:\n",
    "            print(f\"Model: {model.__class__.__name__:20s}  Word: {index2word[idx]:15s}  Similarity: {similarities[idx]:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "\n",
    "input_word = random.choice(vocab)\n",
    "\n",
    "find_top_similar_words(model_skipgram, word2index_skipgram, vocab_skipgram, input_word)\n",
    "find_top_similar_words(model_skipgram_neg, word2index_neg, vocab_neg, input_word)\n",
    "find_top_similar_words(model_glove, word2index_glove, vocab_glove, input_word)\n",
    "find_top_similar_words(model_glove_gensim, model_glove_gensim.key_to_index, None, input_word, is_gensim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram            : 'surpass' is to 'chung' as 'action' is to 'chung'\n",
      "SkipgramNegSampling : 'surpass' is to 'chung' as 'action' is to 'chung'\n",
      "GloVe               : 'surpass' is to 'chung' as 'action' is to 'chung'\n",
      "Gensim              : 'surpass' is to 'chung' as 'action' is to 'called'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# Analogy Prediction Function\n",
    "def predict_analogy(model, word2index, word_a, word_b, word_c, is_gensim=False):\n",
    "    if is_gensim:\n",
    "        if all(word in model for word in [word_a, word_b, word_c]):\n",
    "            result = model.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)[0][0]\n",
    "            print(f\"{'Gensim':<20s}: '{word_a}' is to '{word_b}' as '{word_c}' is to '{result}'\")\n",
    "        else:\n",
    "            print(\"One or more input words are not in the vocabulary.\")\n",
    "    else:\n",
    "        if all(word in word2index for word in [word_a, word_b, word_c]):\n",
    "            a, b, c = [word2index[word] for word in [word_a, word_b, word_c]]\n",
    "            vec = (\n",
    "                model.embedding_v(torch.tensor([b])).detach()\n",
    "                - model.embedding_v(torch.tensor([a])).detach()\n",
    "                + model.embedding_v(torch.tensor([c])).detach()\n",
    "            )\n",
    "            similarities = torch.matmul(model.embedding_v.weight, vec.squeeze())\n",
    "            predicted_idx = torch.argmax(similarities).item()\n",
    "            predicted_word = vocab_glove[predicted_idx]\n",
    "            print(f\"{model.__class__.__name__:<20s}: '{word_a}' is to '{word_b}' as '{word_c}' is to '{predicted_word}'\")\n",
    "        else:\n",
    "            print(\"One or more input words are not in the vocabulary.\")\n",
    "\n",
    "\n",
    "first_word = random.choice(vocab)\n",
    "second_word = random.choice(vocab)\n",
    "third_word = random.choice(vocab)\n",
    "\n",
    "predict_analogy(model_skipgram, word2index_skipgram, first_word, second_word, third_word)\n",
    "predict_analogy(model_skipgram_neg, word2index_neg, first_word, second_word, third_word)\n",
    "predict_analogy(model_glove, word2index_glove, first_word, second_word, third_word)\n",
    "predict_analogy(model_glove_gensim, model_glove_gensim.key_to_index, first_word, second_word, third_word, is_gensim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram            : iida norgold nbh pazzionotto dharyono renouf nzi bearer argyll deadlocked\n",
      "SkipgramNegSampling : iida norgold nbh pazzionotto dharyono renouf nzi bearer argyll deadlocked\n",
      "GloVe               : iida norgold nbh pazzionotto dharyono renouf nzi bearer argyll deadlocked\n",
      "Gensim              : iida takahashi fulbright fellowships makeovers titillation biłgoraj giżycko soldado cassano\n"
     ]
    }
   ],
   "source": [
    "# Function to Generate Sentence from a Single Input Word\n",
    "def generate_sentence(model, word2index, index2word, input_word, length=9, is_gensim=False):\n",
    "    if is_gensim:\n",
    "        if input_word not in model:\n",
    "            print(f\"'{input_word}' not in vocabulary.\")\n",
    "            return\n",
    "        sentence = [input_word]\n",
    "        current_word = input_word\n",
    "        previous_word = np.random.choice(list(model.key_to_index.keys()))\n",
    "        for _ in range(length):\n",
    "            next_word = model.most_similar(positive=[current_word], negative=[previous_word], topn=1)[0][0]\n",
    "            sentence.append(next_word)\n",
    "            previous_word = current_word\n",
    "            current_word = next_word\n",
    "        print(f\"{'Gensim':<20s}:\", \" \".join(sentence))\n",
    "    else:\n",
    "        if input_word not in word2index:\n",
    "            print(f\"'{input_word}' not in vocabulary.\")\n",
    "            return\n",
    "        sentence = [input_word]\n",
    "        current_idx = word2index[input_word]\n",
    "        for _ in range(length):\n",
    "            input_vec = model.embedding_v(torch.tensor([current_idx])).detach()\n",
    "            similarities = torch.matmul(model.embedding_v.weight, input_vec.squeeze())\n",
    "            next_idx = torch.argmax(similarities).item() + 1\n",
    "            next_word = index2word[next_idx]\n",
    "            sentence.append(next_word)\n",
    "            current_idx = next_idx\n",
    "        print(f\"{model.__class__.__name__:<20s}:\", \" \".join(sentence))\n",
    "\n",
    "\n",
    "input_word = random.choice(vocab)\n",
    "\n",
    "generate_sentence(model_skipgram, word2index_skipgram, vocab_skipgram, input_word)\n",
    "generate_sentence(model_skipgram_neg, word2index_neg, vocab_neg, input_word)\n",
    "generate_sentence(model_glove, word2index_glove, vocab_glove, input_word)\n",
    "generate_sentence(model_glove_gensim, model_glove_gensim.key_to_index, None, input_word, is_gensim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram\n",
      "----------------------------------------------------------------------\n",
      "Word1               Word2               Model Score    Human Score    \n",
      "----------------------------------------------------------------------\n",
      "population          development         -0.4404        3.7500         \n",
      "problem             airport             -0.0960        2.3800         \n",
      "investigation       effort              0.5806         4.5900         \n",
      "money               currency            0.0461         9.0400         \n",
      "crane               implement           1.7027         2.6900         \n",
      "**********************************************************************\n",
      "Spearman Correlation: 0.1000\n",
      "**********************************************************************\n",
      "\n",
      "SkipgramNegSampling\n",
      "----------------------------------------------------------------------\n",
      "Word1               Word2               Model Score    Human Score    \n",
      "----------------------------------------------------------------------\n",
      "jaguar              car                 17.0532        7.2700         \n",
      "man                 governor            -15.6424       5.2500         \n",
      "consumer            confidence          9.9341         4.1300         \n",
      "aluminum            metal               21.8943        7.8300         \n",
      "start               match               0.4263         4.4700         \n",
      "**********************************************************************\n",
      "Spearman Correlation: 0.6000\n",
      "**********************************************************************\n",
      "\n",
      "GloVe\n",
      "----------------------------------------------------------------------\n",
      "Word1               Word2               Model Score    Human Score    \n",
      "----------------------------------------------------------------------\n",
      "crane               implement           -0.5555        2.6900         \n",
      "life                term                -0.0016        4.5000         \n",
      "situation           conclusion          -2.5095        4.8100         \n",
      "experience          music               -1.6274        3.4700         \n",
      "phone               equipment           -2.4810        7.1300         \n",
      "**********************************************************************\n",
      "Spearman Correlation: -0.6000\n",
      "**********************************************************************\n",
      "\n",
      "Gensim\n",
      "----------------------------------------------------------------------\n",
      "Word1               Word2               Model Score    Human Score    \n",
      "----------------------------------------------------------------------\n",
      "planet              sun                 0.4861         8.0200         \n",
      "media               trading             0.4104         3.8800         \n",
      "life                death               0.6755         7.8800         \n",
      "bishop              rabbi               0.4402         6.6900         \n",
      "cup                 artifact            -0.0659        2.9200         \n",
      "**********************************************************************\n",
      "Spearman Correlation: 0.9000\n",
      "**********************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_random_similarity(model, word2index, similarity_data, is_gensim=False):\n",
    "    # Extract word pairs that exist in both vocab and similarity dataset\n",
    "    valid_pairs = []\n",
    "    for line in similarity_data:\n",
    "        word1, word2, score = line.strip().split()\n",
    "        if word1 in word2index and word2 in word2index:\n",
    "            valid_pairs.append((word1, word2, float(score)))\n",
    "\n",
    "    # Randomly select 5 pairs\n",
    "    sampled_pairs = random.sample(valid_pairs, 5)\n",
    "\n",
    "    model_scores = []\n",
    "    human_scores = []\n",
    "    results = []\n",
    "\n",
    "    for word1, word2, human_score in sampled_pairs:\n",
    "        if is_gensim:\n",
    "            model_score = model.similarity(word1, word2)\n",
    "        else:\n",
    "            vec1 = model.embedding_v(torch.tensor([word2index[word1]])).detach()\n",
    "            vec2 = model.embedding_v(torch.tensor([word2index[word2]])).detach()\n",
    "            model_score = torch.dot(vec1.squeeze(), vec2.squeeze()).item()\n",
    "\n",
    "        model_scores.append(model_score)\n",
    "        human_scores.append(human_score)\n",
    "        results.append((word1, word2, model_score, human_score))\n",
    "\n",
    "    # Calculate Spearman Correlation\n",
    "    spearman_corr, _ = spearmanr(model_scores, human_scores)\n",
    "\n",
    "    # Display Results\n",
    "    print(f\"{model.__class__.__name__ if not is_gensim else 'Gensim'}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Word1':<20s}{'Word2':<20s}{'Model Score':<15s}{'Human Score':<15s}\")\n",
    "    print(\"-\" * 70)\n",
    "    for word1, word2, model_score, human_score in results:\n",
    "        print(f\"{word1:<20s}{word2:<20s}{model_score:<15.4f}{human_score:<15.4f}\")\n",
    "    print(f\"{'*' * 70}\\nSpearman Correlation: {spearman_corr:.4f}\\n{'*' * 70}\\n\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "evaluate_random_similarity(model_skipgram, word2index_skipgram, similarity_data)\n",
    "evaluate_random_similarity(model_skipgram_neg, word2index_neg, similarity_data)\n",
    "evaluate_random_similarity(model_glove, word2index_glove, similarity_data)\n",
    "evaluate_random_similarity(model_glove_gensim, model_glove_gensim.key_to_index, similarity_data, is_gensim=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
