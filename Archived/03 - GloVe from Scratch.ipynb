{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVE\n",
    "\n",
    "Let's work on implementation of GloVE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Detect the device for computation (CPU/GPU/Metal on Mac ðŸ’»)\n",
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define some very simple data for understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['asian', 'exporters', 'fear', 'damage', 'from', 'u', 's', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u', 's', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asia', 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', 'reaching', 'economic', 'damage', 'businessmen', 'and', 'officials', 'said', 'they', 'told', 'reuter', 'correspondents', 'in', 'asian', 'capitals', 'a', 'u', 's', 'move', 'against', 'japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'u', 's', 'and', 'lead', 'to', 'curbs', 'on', 'american', 'imports', 'of', 'their', 'products', 'but', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long', 'run', 'in', 'the', 'short', 'term', 'tokyo', 's', 'loss', 'might', 'be', 'their', 'gain', 'the', 'u', 's', 'has', 'said', 'it', 'will', 'impose', 'mln', 'dlrs', 'of', 'tariffs', 'on', 'imports', 'of', 'japanese', 'electronics', 'goods', 'on', 'april', 'in', 'retaliation', 'for', 'japan', 's', 'alleged', 'failure', 'to', 'stick', 'to', 'a', 'pact', 'not', 'to', 'sell', 'semiconductors', 'on', 'world', 'markets', 'at', 'below', 'cost', 'unofficial', 'japanese', 'estimates', 'put', 'the', 'impact', 'of', 'the', 'tariffs', 'at', 'billion', 'dlrs', 'and', 'spokesmen', 'for', 'major', 'electronics', 'firms', 'said', 'they', 'would', 'virtually', 'halt', 'exports', 'of', 'products', 'hit', 'by', 'the', 'new', 'taxes', 'we', 'wouldn', 't', 'be', 'able', 'to', 'do', 'business', 'said', 'a', 'spokesman', 'for', 'leading', 'japanese', 'electronics', 'firm', 'matsushita', 'electric', 'industrial', 'co', 'ltd', 'lt', 'mc', 't', 'if', 'the', 'tariffs', 'remain', 'in', 'place', 'for', 'any', 'length', 'of', 'time', 'beyond', 'a', 'few', 'months', 'it', 'will', 'mean', 'the', 'complete', 'erosion', 'of', 'exports', 'of', 'goods', 'subject', 'to', 'tariffs', 'to', 'the', 'u', 's', 'said', 'tom', 'murtha', 'a', 'stock', 'analyst', 'at', 'the', 'tokyo', 'office', 'of', 'broker', 'lt', 'james', 'capel', 'and', 'co', 'in', 'taiwan', 'businessmen', 'and', 'officials', 'are', 'also', 'worried', 'we', 'are', 'aware', 'of', 'the', 'seriousness', 'of', 'the', 'u', 's', 'threat', 'against', 'japan', 'because', 'it', 'serves', 'as', 'a', 'warning', 'to', 'us', 'said', 'a', 'senior', 'taiwanese', 'trade', 'official', 'who', 'asked', 'not', 'to', 'be', 'named', 'taiwan', 'had', 'a', 'trade', 'trade', 'surplus', 'of', 'billion', 'dlrs', 'last', 'year', 'pct', 'of', 'it', 'with', 'the', 'u', 's', 'the', 'surplus', 'helped', 'swell', 'taiwan', 's', 'foreign', 'exchange', 'reserves', 'to', 'billion', 'dlrs', 'among', 'the', 'world', 's', 'largest', 'we', 'must', 'quickly', 'open', 'our', 'markets', 'remove', 'trade', 'barriers', 'and', 'cut', 'import', 'tariffs', 'to', 'allow', 'imports', 'of', 'u', 's', 'products', 'if', 'we', 'want', 'to', 'defuse', 'problems', 'from', 'possible', 'u', 's', 'retaliation', 'said', 'paul', 'sheen', 'chairman', 'of', 'textile', 'exporters', 'lt', 'taiwan', 'safe', 'group', 'a', 'senior', 'official', 'of', 'south', 'korea', 's', 'trade', 'promotion', 'association', 'said', 'the', 'trade', 'dispute', 'between', 'the', 'u', 's', 'and', 'japan', 'might', 'also', 'lead', 'to', 'pressure', 'on', 'south', 'korea', 'whose', 'chief', 'exports', 'are', 'similar', 'to', 'those', 'of', 'japan', 'last', 'year', 'south', 'korea', 'had', 'a', 'trade', 'surplus', 'of', 'billion', 'dlrs', 'with', 'the', 'u', 's', 'up', 'from', 'billion', 'dlrs', 'in', 'in', 'malaysia', 'trade', 'officers', 'and', 'businessmen', 'said', 'tough', 'curbs', 'against', 'japan', 'might', 'allow', 'hard', 'hit', 'producers', 'of', 'semiconductors', 'in', 'third', 'countries', 'to', 'expand', 'their', 'sales', 'to', 'the', 'u', 's', 'in', 'hong', 'kong', 'where', 'newspapers', 'have', 'alleged', 'japan', 'has', 'been', 'selling', 'below', 'cost', 'semiconductors', 'some', 'electronics', 'manufacturers', 'share', 'that', 'view', 'but', 'other', 'businessmen', 'said', 'such', 'a', 'short', 'term', 'commercial', 'advantage', 'would', 'be', 'outweighed', 'by', 'further', 'u', 's', 'pressure', 'to', 'block', 'imports', 'that', 'is', 'a', 'very', 'short', 'term', 'view', 'said', 'lawrence', 'mills', 'director', 'general', 'of', 'the', 'federation', 'of', 'hong', 'kong', 'industry', 'if', 'the', 'whole', 'purpose', 'is', 'to', 'prevent', 'imports', 'one', 'day', 'it', 'will', 'be', 'extended', 'to', 'other', 'sources', 'much', 'more', 'serious', 'for', 'hong', 'kong', 'is', 'the', 'disadvantage', 'of', 'action', 'restraining', 'trade', 'he', 'said', 'the', 'u', 's', 'last', 'year', 'was', 'hong', 'kong', 's', 'biggest', 'export', 'market', 'accounting', 'for', 'over', 'pct', 'of', 'domestically', 'produced', 'exports', 'the', 'australian', 'government', 'is', 'awaiting', 'the', 'outcome', 'of', 'trade', 'talks', 'between', 'the', 'u', 's', 'and', 'japan', 'with', 'interest', 'and', 'concern', 'industry', 'minister', 'john', 'button', 'said', 'in', 'canberra', 'last', 'friday', 'this', 'kind', 'of', 'deterioration', 'in', 'trade', 'relations', 'between', 'two', 'countries', 'which', 'are', 'major', 'trading', 'partners', 'of', 'ours', 'is', 'a', 'very', 'serious', 'matter', 'button', 'said', 'he', 'said', 'australia', 's', 'concerns', 'centred', 'on', 'coal', 'and', 'beef', 'australia', 's', 'two', 'largest', 'exports', 'to', 'japan', 'and', 'also', 'significant', 'u', 's', 'exports', 'to', 'that', 'country', 'meanwhile', 'u', 's', 'japanese', 'diplomatic', 'manoeuvres', 'to', 'solve', 'the', 'trade', 'stand', 'off', 'continue', 'japan', 's', 'ruling', 'liberal', 'democratic', 'party', 'yesterday', 'outlined', 'a', 'package', 'of', 'economic', 'measures', 'to', 'boost', 'the', 'japanese', 'economy', 'the', 'measures', 'proposed', 'include', 'a', 'large', 'supplementary', 'budget', 'and', 'record', 'public', 'works', 'spending', 'in', 'the', 'first', 'half', 'of', 'the', 'financial', 'year', 'they', 'also', 'call', 'for', 'stepped', 'up', 'spending', 'as', 'an', 'emergency', 'measure', 'to', 'stimulate', 'the', 'economy', 'despite', 'prime', 'minister', 'yasuhiro', 'nakasone', 's', 'avowed', 'fiscal', 'reform', 'program', 'deputy', 'u', 's', 'trade', 'representative', 'michael', 'smith', 'and', 'makoto', 'kuroda', 'japan', 's', 'deputy', 'minister', 'of', 'international', 'trade', 'and', 'industry', 'miti', 'are', 'due', 'to', 'meet', 'in', 'washington', 'this', 'week', 'in', 'an', 'effort', 'to', 'end', 'the', 'dispute']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# SAMPLE_SIZE = 100\n",
    "SAMPLE_SIZE = len(reuters.fileids())\n",
    "corpus = []\n",
    "\n",
    "\n",
    "for id in reuters.fileids()[:SAMPLE_SIZE]:\n",
    "    sentences = reuters.words(id)\n",
    "    sentences = [\n",
    "        sentence.lower() for sentence in sentences if sentence.isalpha()\n",
    "    ]\n",
    "    corpus.append(sentences)\n",
    "\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [\n",
    "#     \"apple banana fruit\",\n",
    "#     \"banana apple fruit\",\n",
    "#     \"banana fruit apple\",\n",
    "#     \"dog cat animal\",\n",
    "#     \"cat animal dog\",\n",
    "#     \"cat dog animal\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [sent.split(\" \") for sent in corpus]\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word sequences and unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocab = list(set(flatten(corpus)))\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericalization\n",
    "word2index = {w: i for i, w in enumerate(vocab)}\n",
    "# print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab size\n",
    "voc_size = len(vocab)\n",
    "# print(voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append UNK\n",
    "vocab.append(\"<UNK>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index[\"<UNK>\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just in case we need to use\n",
    "index2word = {v: k for k, v in word2index.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Co-occurence Matrix X\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we need to count the co-occurence of two words given some window size. We gonna use window size of 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "X_i = Counter(flatten(corpus))  # X_i\n",
    "# X_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make skip gram of one size window\n",
    "skip_grams = []\n",
    "# loop each word sequence\n",
    "# we starts from 1 because 0 has no context\n",
    "# we stop at second last for the same reason\n",
    "for sent in corpus:\n",
    "    for i in range(1, len(sent) - 1):\n",
    "        target = sent[i]\n",
    "        context = [sent[i - 1], sent[i + 1]]\n",
    "        for w in context:\n",
    "            skip_grams.append((target, w))\n",
    "\n",
    "# skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik_skipgram = Counter(skip_grams)  # Co-occurece in window size 1\n",
    "# X_ik_skipgram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting function\n",
    "\n",
    "GloVe includes a weighting function to scale down too frequent words.\n",
    "\n",
    "<img src = \"../figures/glove_weighting_func.png\" width=400>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply a normalized function...don't worry too much\n",
    "def weighting(w_i, w_j, X_ik):\n",
    "\n",
    "    # check whether the co-occurrences exist between these two words\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1  # if does not exist, set it to 1\n",
    "\n",
    "    x_max = 100  # 100 # fixed in paper  #cannot exceed 100 counts\n",
    "    alpha = 0.75\n",
    "\n",
    "    # if co-occurrence does not exceed 100, scale it based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij / x_max) ** alpha  # scale it\n",
    "    else:\n",
    "        result = 1  # if is greater than max, set it to 1 maximum\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {}  # for keeping the co-occurences\n",
    "weighting_dic = {}  # scaling the percentage of sampling\n",
    "\n",
    "# Use a generator to avoid creating a large list in memory\n",
    "for bigram in combinations_with_replacement(vocab, 2):\n",
    "    co_occer = X_ik_skipgram.get(bigram, 0)  # get the count from what we already counted, default to 0\n",
    "    if co_occer > 0:\n",
    "        X_ik[bigram] = co_occer + 1  # + 1 for stability issue\n",
    "        X_ik[(bigram[1], bigram[0])] = co_occer + 1  # count also for the opposite\n",
    "\n",
    "    weight = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[bigram] = weight\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weight\n",
    "\n",
    "    # Clear RAM\n",
    "    gc.collect()\n",
    "\n",
    "# print(f\"{X_ik=}\")\n",
    "# print(f\"{weighting_dic=}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in corpus:\n",
    "#     print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "\n",
    "    # convert to id since our skip_grams is word, not yet id\n",
    "    skip_grams_id = [\n",
    "        (word2index[skip_gram[0]], word2index[skip_gram[1]])\n",
    "        for skip_gram in skip_grams\n",
    "    ]\n",
    "\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_coocs = []\n",
    "    random_weightings = []\n",
    "    random_index = np.random.choice(\n",
    "        range(len(skip_grams_id)), batch_size, replace=False\n",
    "    )  # randomly pick without replacement\n",
    "\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams_id[i][1]])  # context word, e.g., 3\n",
    "\n",
    "        # get cooc\n",
    "        pair = skip_grams[i]\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "\n",
    "        # get weighting\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "\n",
    "    return (\n",
    "        np.array(random_inputs),\n",
    "        np.array(random_labels),\n",
    "        np.array(random_coocs),\n",
    "        np.array(random_weightings),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the method\n",
    "batch_size = 2  # mini-batch size\n",
    "input_batch, target_batch, cooc_batch, weighting_batch = random_batch(\n",
    "    batch_size, corpus, skip_grams, X_ik, weighting_dic\n",
    ")\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)\n",
    "print(\"Cooc: \", cooc_batch)\n",
    "print(\"Weighting: \", weighting_batch)\n",
    "\n",
    "# we will convert them to tensor during training, so don't worry..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model\n",
    "\n",
    "<img src =\"../figures/glove.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(GloVe, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(\n",
    "            vocab_size, embed_size\n",
    "        )  # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size)  # out embedding\n",
    "\n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        center_embeds = self.embedding_v(\n",
    "            center_words\n",
    "        )  # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_u(\n",
    "            target_words\n",
    "        )  # [batch_size, 1, emb_size]\n",
    "\n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "\n",
    "        inner_product = target_embeds.bmm(\n",
    "            center_embeds.transpose(1, 2)\n",
    "        ).squeeze(2)\n",
    "        # [batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "\n",
    "        # note that coocs already got log\n",
    "        loss = weighting * torch.pow(\n",
    "            inner_product + center_bias + target_bias - coocs, 2\n",
    "        )\n",
    "\n",
    "        return torch.sum(loss)\n",
    "\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = GloVe(voc_size, embedding_size).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256  # mini-batch size\n",
    "embedding_size = 100  # so we can later plot\n",
    "model = GloVe(voc_size, embedding_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Training\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch(\n",
    "        batch_size, corpus, skip_grams, X_ik, weighting_dic\n",
    "    )\n",
    "    input_batch = torch.LongTensor(input_batch).to(device)  # [batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch).to(device)  # [batch_size, 1]\n",
    "    cooc_batch = torch.FloatTensor(cooc_batch).to(device)  # [batch_size, 1]\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch).to(\n",
    "        device\n",
    "    )  # [batch_size, 1]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"glove.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plotting the embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of vocabs\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericalization\n",
    "id = word2index[word]\n",
    "# id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tensor = torch.LongTensor([id]).to(device)\n",
    "# id_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embedding by averaging\n",
    "v_embed = model.embedding_v(id_tensor)\n",
    "u_embed = model.embedding_u(id_tensor)\n",
    "\n",
    "# v_embed, u_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average to get the word embedding\n",
    "word_embed = (v_embed + u_embed) / 2\n",
    "word_embed[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write a function to get embedding given a word\n",
    "def get_embed(word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]]).to(device)\n",
    "    v_embed = model.embedding_v(id_tensor)\n",
    "    u_embed = model.embedding_u(id_tensor)\n",
    "    word_embed = (v_embed + u_embed) / 2\n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "for i, word in enumerate(vocab[:20]):  # loop each unique vocab\n",
    "    x, y = get_embed(word)\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords=\"offset points\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cosine similarity\n",
    "\n",
    "Formally the [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) $s$ between two vectors $p$ and $q$ is defined as:\n",
    "\n",
    "$$s = \\frac{p \\cdot q}{||p|| ||q||}, \\textrm{ where } s \\in [-1, 1] $$\n",
    "\n",
    "If $p$ and $q$ is super similar, the result is 1 otherwise 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try similarity between first and second, and second and third\n",
    "cat = get_embed(\"cat\")\n",
    "fruit = get_embed(\"fruit\")\n",
    "animal = get_embed(\"animal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy version\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b) / (norm(a) * norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "\n",
    "print(f\"cat vs. fruit: \", cos_sim(cat, fruit))\n",
    "print(f\"cat vs. animal: \", cos_sim(cat, animal))\n",
    "print(f\"cat vs. cat: \", cos_sim(cat, cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy version\n",
    "from scipy import spatial\n",
    "\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = 1 - spatial.distance.cosine(\n",
    "        a, b\n",
    "    )  # distance = 1 - similarlity, because scipy only gives distance\n",
    "    return cos_sim\n",
    "\n",
    "\n",
    "print(f\"cat vs. fruit: \", cos_sim(cat, fruit))\n",
    "print(f\"cat vs. animal: \", cos_sim(cat, animal))\n",
    "print(f\"cat vs. cat: \", cos_sim(cat, cat))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "nlp-a1",
   "language": "python",
   "name": "nlp-a1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
