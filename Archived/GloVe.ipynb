{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/silanm/Developer/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/silanm/Developer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 32895.9103, Time: 7.85s\n",
      "Epoch 2, Loss: 5698.0985, Time: 7.67s\n",
      "Epoch 3, Loss: 2316.7251, Time: 7.61s\n",
      "Epoch 4, Loss: 1125.0549, Time: 7.65s\n",
      "Epoch 5, Loss: 601.5070, Time: 7.60s\n",
      "Epoch 6, Loss: 344.5856, Time: 7.63s\n",
      "Epoch 7, Loss: 211.7330, Time: 7.64s\n",
      "Epoch 8, Loss: 140.3280, Time: 7.62s\n",
      "Epoch 9, Loss: 101.4641, Time: 7.61s\n",
      "Epoch 10, Loss: 79.2047, Time: 7.56s\n",
      "Model and vocabulary saved to glove_model.pth\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import reuters\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Data Preparation\n",
    "sample_size = 1000\n",
    "window_size = 2\n",
    "min_word_freq = 5  # Frequency threshold for vocabulary trimming\n",
    "\n",
    "\n",
    "def build_corpus():\n",
    "    corpus = []\n",
    "    for file_id in reuters.fileids()[:sample_size]:\n",
    "        sentences = reuters.words(file_id)\n",
    "        sentences = [word.lower() for word in sentences if word.isalpha()]\n",
    "        corpus.append(sentences)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def build_vocab(corpus):\n",
    "    words = [word for sentence in corpus for word in sentence]\n",
    "    word_counts = Counter(words)\n",
    "    vocab = [word for word, count in word_counts.items() if count >= min_word_freq]\n",
    "    vocab.append(\"<UNKNOWN>\")\n",
    "    word2index = {word: idx for idx, word in enumerate(vocab)}\n",
    "    word2index[\"<UNKNOWN>\"] = 0\n",
    "    return vocab, len(vocab), word2index, word_counts\n",
    "\n",
    "\n",
    "def build_skipgrams(corpus, word2index, window_size):\n",
    "    skip_grams = []\n",
    "    for sentence in corpus:\n",
    "        for pos, center_word in enumerate(sentence):\n",
    "            center_idx = word2index.get(center_word, word2index[\"<UNKNOWN>\"])\n",
    "            context_indices = [\n",
    "                word2index.get(sentence[i], word2index[\"<UNKNOWN>\"])\n",
    "                for i in range(max(pos - window_size, 0), min(pos + window_size + 1, len(sentence)))\n",
    "                if i != pos\n",
    "            ]\n",
    "            for context_idx in context_indices:\n",
    "                skip_grams.append((center_idx, context_idx))\n",
    "    return skip_grams\n",
    "\n",
    "\n",
    "def weighting_function(x_ij, x_max=100, alpha=0.75):\n",
    "    return (x_ij / x_max) ** alpha if x_ij < x_max else 1\n",
    "\n",
    "\n",
    "# GloVe Model\n",
    "class GloVe(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(GloVe, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size)\n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "    def forward(self, center_words, target_words, co_occurrences, weightings):\n",
    "        center_embed = self.embedding_v(center_words)\n",
    "        target_embed = self.embedding_u(target_words)\n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "        inner_product = (center_embed * target_embed).sum(dim=1)\n",
    "        loss = weightings * torch.pow(inner_product + center_bias + target_bias - co_occurrences, 2)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def prepare_training_data(skip_grams, co_occurrence_matrix, word2index):\n",
    "    training_data = []\n",
    "    for center, context in skip_grams:\n",
    "        co_occurrence = co_occurrence_matrix.get((center, context), 1)\n",
    "        weight = weighting_function(co_occurrence)\n",
    "        training_data.append((center, context, math.log(co_occurrence + 1), weight))\n",
    "    return training_data\n",
    "\n",
    "\n",
    "# Training Function\n",
    "def train_glove_model(model, training_data, epochs, batch_size, learning_rate):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "        random.shuffle(training_data)\n",
    "        for i in range(0, len(training_data), batch_size):\n",
    "            batch = training_data[i : i + batch_size]\n",
    "            centers, contexts, coocs, weights = zip(*batch)\n",
    "            centers = torch.LongTensor(centers).to(device)\n",
    "            contexts = torch.LongTensor(contexts).to(device)\n",
    "            coocs = torch.FloatTensor(coocs).to(device)\n",
    "            weights = torch.FloatTensor(weights).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(centers, contexts, coocs, weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}, Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "\n",
    "# Main Execution\n",
    "corpus = build_corpus()\n",
    "vocab, vocab_size, word2index, word_counts = build_vocab(corpus)\n",
    "skip_grams = build_skipgrams(corpus, word2index, window_size)\n",
    "co_occurrence_matrix = Counter(skip_grams)\n",
    "training_data = prepare_training_data(skip_grams, co_occurrence_matrix, word2index)\n",
    "\n",
    "embedding_dim = 100\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = GloVe(vocab_size, embedding_dim).to(device)\n",
    "train_glove_model(model, training_data, epochs, batch_size, learning_rate)\n",
    "\n",
    "# Save the model\n",
    "torch.save({\"model_state_dict\": model.state_dict(), \"word2index\": word2index, \"vocab\": vocab}, \"glove_model.pth\")\n",
    "print(\"Model and vocabulary saved to glove_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
