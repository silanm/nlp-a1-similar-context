{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect the device for computation (CPU/GPU/Metal on Mac ðŸ’»)\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure required datasets are downloaded\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 500\n",
    "# sample_size = len(reuters.fileids())\n",
    "\n",
    "\n",
    "def build_corpus():\n",
    "    corpus = []\n",
    "\n",
    "    for id in reuters.fileids()[:sample_size]:\n",
    "        sentences = reuters.words(id)\n",
    "        sentences = [sentence.lower() for sentence in sentences if sentence.isalpha()]\n",
    "        corpus.append(sentences)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "corpus = build_corpus()\n",
    "print(f\"Sentences in the corpus: {len(corpus)}\")\n",
    "print(f\"Words in the corpus: {sum(len(sentence) for sentence in corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(corpus):\n",
    "    # Flatten words from all sentences\n",
    "    flatten_words = [word for sentence in corpus for word in sentence]\n",
    "    word_counts = Counter(flatten_words)\n",
    "\n",
    "    # Unique words\n",
    "    vocab = list(set(flatten_words))\n",
    "    vocab.append(\"<UNKNOWN>\")\n",
    "\n",
    "    word2index = {word: index for index, word in enumerate(vocab)}\n",
    "    word2index[\"<UNKNOWN>\"] = 0\n",
    "    word2index\n",
    "\n",
    "    return vocab, len(vocab), word2index, word_counts\n",
    "\n",
    "\n",
    "all_vocabs, vocab_size, word2index, word_counts = build_vocab(corpus)\n",
    "print(f\"Unique words in the corpus: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_skipgrams(corpus, word2index, window_size=2):\n",
    "    \"\"\"\n",
    "    Generate skip-gram pairs from corpus\n",
    "    \"\"\"\n",
    "    skip_grams = []\n",
    "    skip_grams_words = []\n",
    "\n",
    "    for sentence in corpus:\n",
    "        for position, center_word in enumerate(sentence):\n",
    "            center_index = word2index[center_word]\n",
    "            context_indices = list(\n",
    "                [\n",
    "                    i\n",
    "                    for i in range(\n",
    "                        max(\n",
    "                            position - window_size, 0\n",
    "                        ),  # Context words on the left. If none, then 0\n",
    "                        min(\n",
    "                            position + window_size + 1, len(sentence)\n",
    "                        ),  # Context words on the right, If none, then 0\n",
    "                    )\n",
    "                    if i != position  # Exclude itself\n",
    "                ]\n",
    "            )\n",
    "            for index in context_indices:\n",
    "                context_word = sentence[index]\n",
    "                context_index = word2index[context_word]\n",
    "                skip_grams.append(\n",
    "                    (center_index, context_index)\n",
    "                )  # A tuple representing a skip-gram pair (indices)\n",
    "                skip_grams_words.append(\n",
    "                    (center_word, context_word)\n",
    "                )  # A tuple representing a skip-gram pair (words)\n",
    "\n",
    "    return skip_grams, skip_grams_words\n",
    "\n",
    "\n",
    "skip_grams, skip_grams_words = build_skipgrams(corpus, word2index, window_size=2)\n",
    "print(f\"Skip-gram pairs from corpus: {len(skip_grams)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigram_table(word_counts, power=0.75):\n",
    "    total_count = sum([count for count in word_counts.values()])\n",
    "    Z = 0.001\n",
    "    unigram_table = []\n",
    "\n",
    "    for word, count in word_counts.items():\n",
    "        # score = (count / total_count) ** power\n",
    "        # unigram_table.extend([word] * int(score * 1e6))\n",
    "        unigram_table.extend([word] * int(((count / total_count) ** power) / Z))\n",
    "    return unigram_table\n",
    "\n",
    "\n",
    "unigram_table = build_unigram_table(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_number_sequence(all_vocabs, word2index):\n",
    "    \"\"\"\n",
    "    Convert a sequence of words into a sequence of numerical indices\n",
    "    \"\"\"\n",
    "    indices = list(\n",
    "        map(  # Apply lambda function to each word in all_vocabs\n",
    "            lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNKNOWN>\"],\n",
    "            all_vocabs,\n",
    "        )\n",
    "    )\n",
    "    return torch.LongTensor(indices).to(device)  # List of indices is converted to PyTorch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurrence + Weighting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_ik_skipgram = Counter(skip_grams_words)\n",
    "# X_ik_skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weighting(word_i, word_j, X_ik):\n",
    "#     try:\n",
    "#         x_ij = X_ik[(word_i, word_j)]\n",
    "#     except:\n",
    "#         x_ij = 1\n",
    "\n",
    "#     x_max = 100\n",
    "#     alpha = 0.75\n",
    "\n",
    "#     if x_ij < x_max:\n",
    "#         result = (x_ij / x_max) ** alpha\n",
    "#     else:\n",
    "#         result = 1\n",
    "\n",
    "#     return result\n",
    "\n",
    "\n",
    "# from itertools import combinations_with_replacement\n",
    "\n",
    "# X_ik = {}\n",
    "# weighting_dict = {}\n",
    "\n",
    "# for bi_gram in combinations_with_replacement(all_vocabs, 2):\n",
    "#     if X_ik_skipgram.get(bi_gram) is not None:\n",
    "#         co_occurrence = X_ik_skipgram[bi_gram]\n",
    "#         X_ik[bi_gram] = co_occurrence + 1\n",
    "#         X_ik[(bi_gram[1], bi_gram[0])] = co_occurrence + 1  # Opposite\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "#     weighting_dict[bi_gram] = weighting(bi_gram[0], bi_gram[1], X_ik)\n",
    "#     weighting_dict[(bi_gram[1], bi_gram[0])] = weighting(bi_gram[1], bi_gram[0], X_ik)  # Opposite\n",
    "\n",
    "# print(f\"{X_ik=}\")\n",
    "# print(f\"{weighting_dict=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):  # nn.Module is the base class for all neural network modules in PyTorch\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, mode=\"softmax\"):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, center_words, target_words, all_vocabs, negative_words=None):\n",
    "        # Create embedding vectors for center words, target words, and all words\n",
    "        center_embeds = self.embedding_v(center_words)\n",
    "        target_embeds = self.embedding_u(target_words)\n",
    "        all_embeds = self.embedding_u(all_vocabs)\n",
    "\n",
    "        if self.mode == \"softmax\":\n",
    "            # Dot product between the embeddings of the center word and the context word is computed.\n",
    "            # This measures how similar the center word is to the context word.\n",
    "            scores = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "            # Dot product between the embeddings of the center word and all words in the vocabulary is computed.\n",
    "            # This is used to normalize the scores across the entire vocabulary (denominator in the softmax function).\n",
    "            norm_scores = all_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "            # Negative log of the softmax probability is taken. This is the loss for a single prediction.\n",
    "            # The overall loss is the average of these values across all predictions in the batch.\n",
    "            negative_log_likelihood = (-1) * (\n",
    "                torch.mean(\n",
    "                    torch.log(torch.exp(scores) / torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))\n",
    "                )\n",
    "            )\n",
    "        elif self.mode == \"negative_sampling\":\n",
    "            # Create embedding vectors for negative words\n",
    "            negative_embeds = self.embedding_u(negative_words)\n",
    "\n",
    "            # Compute the dot product between center and target word embeddings\n",
    "            # positive_socre = torch.sum(center_embeds * target_embeds, dim=1)\n",
    "            positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "            # Compute the dot product between center and negative word embeddings\n",
    "            # negative_score = torch.bmm(negative_embeds, center_embeds.unsqueeze(2)).squeeze()\n",
    "            negative_score = negative_embeds.bmm(center_embeds.transpose(1, 2))\n",
    "\n",
    "            # Compute the positive loss using the sigmoid function\n",
    "            positive_loss = torch.log(torch.sigmoid(positive_score))\n",
    "            # Compute the negative loss using the sigmoid function\n",
    "            negative_loss = torch.sum(torch.log(torch.sigmoid(-negative_score)), dim=1)\n",
    "\n",
    "            # Compute the negative log likelihood as the mean of the positive and negative losses\n",
    "            negative_log_likelihood = -torch.mean(positive_loss + negative_loss)\n",
    "\n",
    "        return negative_log_likelihood\n",
    "\n",
    "    def get_embedding(self, word_index):\n",
    "        return (\n",
    "            self.embedding_v(torch.tensor([word_index], dtype=torch.long).to(device))\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "        # return self.embedding_v(torch.LongTensor([word_index]).to(device)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(GloVe, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size)\n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "    def forward(self, center_words, target_words, co_occurrences, weighting):\n",
    "        center_embeds = self.embedding_v(center_words)\n",
    "        target_embeds = self.embedding_u(target_words)\n",
    "\n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "\n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        loss = weighting * torch.power((inner_product + center_bias + target_bias), 2)\n",
    "\n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "negative_size = 5\n",
    "embed_size = 100\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "def train_skipgrams(mode, skip_grams, all_vocabs, vocab_size):\n",
    "    if mode not in [\"softmax\", \"negative_sampling\"]:\n",
    "        raise ValueError(\"Invalid mode.\")\n",
    "\n",
    "    # Model and Optimizer\n",
    "    model = Skipgram(vocab_size=vocab_size, embed_size=embed_size, mode=mode).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Convert all_vocabs (list -> tensor) and expands it to match the batch size\n",
    "    all_vocabs = to_number_sequence(all_vocabs, word2index).expand(batch_size, vocab_size)\n",
    "\n",
    "    all_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = []  # Loss for each epoch\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Shuffle the skip_grams every epoch\n",
    "        random.shuffle(skip_grams)\n",
    "\n",
    "        for i in range(0, len(skip_grams), batch_size):\n",
    "            current_batch = skip_grams[i : i + batch_size]\n",
    "\n",
    "            if len(current_batch) == 0:\n",
    "                continue\n",
    "\n",
    "            # Unpack current_batch by splitting into two variables\n",
    "            # center_words = all first elements from each pair\n",
    "            # target_words = all second elements from each pair\n",
    "            center_words, target_words = zip(*current_batch)\n",
    "\n",
    "            # Convert from tuples to 2D numpy arrays\n",
    "            center_words = np.array(center_words).reshape(-1, 1)\n",
    "            target_words = np.array(target_words).reshape(-1, 1)\n",
    "\n",
    "            # Convert from 2D numpy arrays to tensors\n",
    "            center_words = torch.LongTensor(center_words).to(device)\n",
    "            target_words = torch.LongTensor(target_words).to(device)\n",
    "\n",
    "            # RuntimeError: Expected size for first two dimensions of batch2 tensor to be: [128, 200] but got: [124, 200]\n",
    "            # Solution: Pad the batch if it's smaller than batch_size\n",
    "            if center_words.size(0) < batch_size:\n",
    "                padding_size = batch_size - center_words.size(0)\n",
    "                center_words = torch.cat(\n",
    "                    [center_words, torch.zeros(padding_size, 1, dtype=torch.long).to(device)], dim=0\n",
    "                ).to(device)\n",
    "                target_words = torch.cat(\n",
    "                    [target_words, torch.zeros(padding_size, 1, dtype=torch.long).to(device)], dim=0\n",
    "                ).to(device)\n",
    "\n",
    "            # Reset the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if mode == \"softmax\":\n",
    "                # Computes the loss using model's forward method\n",
    "                loss = model(center_words, target_words, all_vocabs)\n",
    "            elif mode == \"negative_sampling\":\n",
    "                negative_words = []\n",
    "                for _ in range(len(center_words)):\n",
    "                    negative_samples = []\n",
    "                    while len(negative_samples) < negative_size:\n",
    "                        word = random.choice(unigram_table)\n",
    "                        if word2index[word] not in target_words:\n",
    "                            negative_samples.append(word2index[word])\n",
    "                    negative_words.append(negative_samples)\n",
    "                negative_words = torch.LongTensor(negative_words).to(device)\n",
    "                # print(negative_words.size())\n",
    "\n",
    "                loss = model(center_words, target_words, all_vocabs, negative_words)\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "            # Updates model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(\n",
    "            #     f\"Epoch {epoch+1} | Batch {i+1}/{(len(skip_grams)+1)} | Center {center_words.size()} | Target {target_words.size()} | Loss {loss.item()}\"\n",
    "            # )\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch: {epoch+1:3d} | Loss: {np.mean(epoch_loss):5.5f} | Time: {elapsed_time:5.5f}\")\n",
    "        all_losses.append(np.mean(epoch_loss))\n",
    "\n",
    "    return model, np.mean(all_losses)\n",
    "\n",
    "\n",
    "def train_glove():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_sm, avg_loss_sm = train_skipgrams(\n",
    "    mode=\"softmax\", skip_grams=skip_grams, all_vocabs=all_vocabs, vocab_size=len(all_vocabs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ng, avg_loss_ng = train_skipgrams(\n",
    "    mode=\"negative_sampling\",\n",
    "    skip_grams=skip_grams,\n",
    "    all_vocabs=all_vocabs,\n",
    "    vocab_size=len(all_vocabs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipgram_sm_path = f\"model_skipgram_sm_b{batch_size}_em{embed_size}_ep{epochs}.pth\"\n",
    "model_skipgram_ng_path = (\n",
    "    f\"model_skipgram_ng_b{batch_size}_em{embed_size}_ep{epochs}_neg{negative_size}.pth\"\n",
    ")\n",
    "\n",
    "torch.save(model_sm, model_skipgram_sm_path)\n",
    "torch.save(model_ng, model_skipgram_ng_path)\n",
    "\n",
    "print(\"Models saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipgram_sm_path = f\"model_skipgram_sm_b{batch_size}_em{embed_size}_ep{epochs}.pth\"\n",
    "model_skipgram_ng_path = (\n",
    "    f\"model_skipgram_ng_b{batch_size}_em{embed_size}_ep{epochs}_neg{negative_size}.pth\"\n",
    ")\n",
    "\n",
    "loaded_model_sm = Skipgram(vocab_size=len(all_vocabs), embed_size=embed_size, mode=\"softmax\").to(\n",
    "    device\n",
    ")\n",
    "loaded_model_sm.load_state_dict(torch.load(model_skipgram_sm_path, map_location=device))\n",
    "\n",
    "loaded_model_ng = Skipgram(\n",
    "    vocab_size=len(all_vocabs), embed_size=embed_size, mode=\"negative_sampling\"\n",
    ").to(device)\n",
    "loaded_model_ng.load_state_dict(torch.load(model_skipgram_ng_path, map_location=device))\n",
    "\n",
    "print(\"Models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(model, word):\n",
    "    # Convert word to tensor of its index\n",
    "    id_tensor = torch.LongTensor([word2index[word]]).to(device)\n",
    "    # Get the embedding vectors for the word from both embedding matrices\n",
    "    v_embed = model.embedding_v(id_tensor)\n",
    "    u_embed = model.embedding_u(id_tensor)\n",
    "    # Average the two embedding vectors to get the final word embedding\n",
    "    word_embed = (v_embed + u_embed) / 2\n",
    "    # Extract the x and y coordinates from the embedding\n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "for i, word in enumerate(all_vocabs[30:50]):  # loop each unique vocab\n",
    "    x, y = get_embed(loaded_model_sm, word)\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords=\"offset points\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "for i, word in enumerate(all_vocabs[30:50]):  # loop each unique vocab\n",
    "    x, y = get_embed(loaded_model_ng, word)\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords=\"offset points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram Analogies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_analogies(model, analogy_file, word2index):\n",
    "    # Initialize counters for correct and total predictions\n",
    "    semantic_correct = 0\n",
    "    semantic_total = 0\n",
    "    syntactic_correct = 0\n",
    "    syntactic_total = 0\n",
    "\n",
    "    # Create a reverse mapping from index to word\n",
    "    index2word = {val: key for key, val in word2index.items()}\n",
    "\n",
    "    section = None\n",
    "\n",
    "    with open(analogy_file, \"r\") as f:\n",
    "        # Lines starting with \":\" indicate a new section\n",
    "        for line in f:\n",
    "            if line.startswith(\":\"):\n",
    "                section = line.strip()[1:].lower()\n",
    "                continue\n",
    "\n",
    "            # Split each line into words and convert to lowercase\n",
    "            words_in_line = line.strip().lower().split()\n",
    "            # Check if all words are present in word2index\n",
    "            if all(word in word2index for word in words_in_line):\n",
    "                first_word, second_word, third_word, expected_word = words_in_line\n",
    "                # Get embeddings for the words\n",
    "                first_word_emb = model.get_embedding(word2index[first_word])\n",
    "                second_word_vec = model.get_embedding(word2index[second_word])\n",
    "                third_word_vec = model.get_embedding(word2index[third_word])\n",
    "\n",
    "                # Find the word in the vocabulary whose embedding is closest to the predicted vector\n",
    "                predicted_idx = np.argmax(\n",
    "                    np.dot(\n",
    "                        model.embedding_v.weight.detach().cpu().numpy(),\n",
    "                        (second_word_vec - first_word_emb + third_word_vec).T,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Retrieve word using predicted index\n",
    "                predicted_word = index2word[predicted_idx]\n",
    "\n",
    "                # Check if the section is semantic or syntactic and update counters\n",
    "                if section == \"capital-common-countries\":\n",
    "                    semantic_total += 1\n",
    "                    if predicted_word == expected_word:\n",
    "                        semantic_correct += 1\n",
    "\n",
    "                elif section == \"past-tense\":\n",
    "                    syntactic_total += 1\n",
    "                    if predicted_word == expected_word:\n",
    "                        syntactic_correct += 1\n",
    "\n",
    "    # Calculate accuracy ratios\n",
    "    semantic_accuracy = semantic_correct / semantic_total if semantic_total > 0 else 0\n",
    "    syntactic_accuracy = syntactic_correct / syntactic_total if syntactic_total > 0 else 0\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Model: {model.mode}\")\n",
    "    print(f\"  Semantic Accuracy: {semantic_accuracy:.4f}\")\n",
    "    print(f\"  Syntactic Accuracy: {syntactic_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_file = \"word-test.v1.txt\"\n",
    "\n",
    "evaluate_analogies(loaded_model_sm, analogy_file, word2index)\n",
    "evaluate_analogies(loaded_model_ng, analogy_file, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogy(model, word_a, word_b, word_c, word2index, index2word):\n",
    "    if word_a not in word2index or word_b not in word2index or word_c not in word2index:\n",
    "        return \"One or more words are not in the vocabulary.\"\n",
    "\n",
    "    # Get embeddings for the words\n",
    "    word_a_emb = model.get_embedding(word2index[word_a])\n",
    "    word_b_emb = model.get_embedding(word2index[word_b])\n",
    "    word_c_emb = model.get_embedding(word2index[word_c])\n",
    "\n",
    "    # Find the word in the vocabulary whose embedding is closest to the predicted vector\n",
    "    predicted_idx = np.argmax(\n",
    "        np.dot(\n",
    "            model.embedding_v.weight.detach().cpu().numpy(),\n",
    "            (word_b_emb - word_a_emb + word_c_emb).T,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Retrieve word using predicted index\n",
    "    predicted_word = index2word[predicted_idx]\n",
    "\n",
    "    return predicted_word\n",
    "\n",
    "\n",
    "# Create a reverse mapping from index to word\n",
    "index2word = {val: key for key, val in word2index.items()}\n",
    "\n",
    "# Test samples from Reuters corpus\n",
    "samples = [\n",
    "    (\"harry\", \"potter\", \"making\"),\n",
    "]\n",
    "\n",
    "for word_a, word_b, word_c in samples:\n",
    "    result_sm = find_analogy(loaded_model_sm, word_a, word_b, word_c, word2index, index2word)\n",
    "    result_ng = find_analogy(loaded_model_ng, word_a, word_b, word_c, word2index, index2word)\n",
    "    print(f\"If {word_a} is to {word_b}, then {word_c} is to {result_sm} (softmax)\")\n",
    "    print(f\"If {word_a} is to {word_b}, then {word_c} is to {result_ng} (negative sampling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram Similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_similarities(model, similarity_file, word2index):\n",
    "    \"\"\"Evaluate similarity correlation using Spearman correlation.\"\"\"\n",
    "    human_scores = []  # List to store human similarity scores\n",
    "    model_scores = []  # List to store model similarity scores\n",
    "\n",
    "    # Open the similarity file and read line by line\n",
    "    with open(similarity_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Split each line into word1, word2, and human_score\n",
    "            word1, word2, human_score = line.strip().split()\n",
    "            # Check if both words are in the word2index dictionary\n",
    "            if word1 in word2index and word2 in word2index:\n",
    "                # Get the embeddings for both words\n",
    "                word1_vec = model.get_embedding(word2index[word1]).squeeze()\n",
    "                word2_vec = model.get_embedding(word2index[word2]).squeeze()\n",
    "\n",
    "                # Calculate the similarity score using dot product\n",
    "                model_score = np.dot(word1_vec, word2_vec)\n",
    "                # Append the human score and model score to their respective lists\n",
    "                human_scores.append(float(human_score))\n",
    "                model_scores.append(model_score)\n",
    "\n",
    "    # Calculate the Spearman correlation between human scores and model scores\n",
    "    correlation, _ = spearmanr(human_scores, model_scores)\n",
    "    # Calculate the mean squared error between human scores and model scores\n",
    "    mse = np.mean((np.array(human_scores) - np.array(model_scores)) ** 2)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Model: {model.mode}\")\n",
    "    print(f\"  Spearman Correlation: {correlation:.4f}\")\n",
    "    print(f\"  MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_file = \"wordsim_similarity_goldstandard.txt\"\n",
    "\n",
    "evaluate_similarities(loaded_model_sm, similarity_file, word2index)\n",
    "evaluate_similarities(loaded_model_ng, similarity_file, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_similarity(model, word1, word2, similarity_file, word2index):\n",
    "    \"\"\"Compare similarity score of two words with human score.\"\"\"\n",
    "    # Get embeddings for both words\n",
    "    word1_vec = model.get_embedding(word2index[word1]).squeeze()\n",
    "    word2_vec = model.get_embedding(word2index[word2]).squeeze()\n",
    "\n",
    "    # Calculate the similarity score using dot product\n",
    "    model_score = np.dot(word1_vec, word2_vec)\n",
    "\n",
    "    # Read the human score from the similarity file\n",
    "    human_score = None\n",
    "    with open(similarity_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            w1, w2, score = line.strip().split()\n",
    "            if (w1 == word1 and w2 == word2) or (w1 == word2 and w2 == word1):\n",
    "                human_score = float(score)\n",
    "                break\n",
    "\n",
    "    if human_score is None:\n",
    "        return f\"No human score found for words: {word1}, {word2}\"\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Model: {model.mode}\")\n",
    "    print(f\"  Words: {word1}, {word2}\")\n",
    "    print(f\"  Model Similarity Score: {model_score:.4f}\")\n",
    "    print(f\"  Human Similarity Score: {human_score:.4f}\")\n",
    "    print(f\"  Difference: {abs(model_score - human_score):.4f}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Read the similarity file and extract word pairs\n",
    "with open(similarity_file, \"r\") as f:\n",
    "    word_pairs = [line.strip().split()[:2] for line in f]\n",
    "\n",
    "# Randomly pick two words from the similarity file and ensure they exist in word2index\n",
    "while True:\n",
    "    word1, word2 = random.choice(word_pairs)\n",
    "    if word1 in word2index and word2 in word2index:\n",
    "        break\n",
    "\n",
    "compare_similarity(loaded_model_sm, word1, word2, similarity_file, word2index)\n",
    "compare_similarity(loaded_model_ng, word1, word2, similarity_file, word2index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
