{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_corpus():\n",
    "    corpus = []\n",
    "    for id in reuters.fileids():\n",
    "        words = reuters.words(id)\n",
    "        words = [word.lower() for word in words if word.isalpha()]\n",
    "        corpus.append(words)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "corpus = build_corpus()\n",
    "len(corpus)  # 10788 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29174"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocab(corpus):\n",
    "    # Flatten words from all sentences\n",
    "    flatten_words = [word for sentence in corpus for word in sentence]\n",
    "    word_counts = Counter(flatten_words)\n",
    "\n",
    "    # Unique words\n",
    "    vocab = list(set(flatten_words))\n",
    "    vocab.append(\"<UNKNOWN>\")\n",
    "\n",
    "    word2index = {word: index for index, word in enumerate(vocab)}\n",
    "    # word2index['<UNKNOWN>'] = 0\n",
    "    # word2index\n",
    "\n",
    "    return vocab, len(vocab), word2index, word_counts\n",
    "\n",
    "\n",
    "vocab, vocab_size, word2index, word_counts = build_vocab(corpus)\n",
    "vocab_size  # 29174 unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5243836"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_skipgrams(corpus, word2index, window_size=2):\n",
    "    skip_grams = []\n",
    "\n",
    "    for sentence in corpus:\n",
    "        for position, center_word in enumerate(sentence):\n",
    "            center_index = word2index[center_word]\n",
    "            context_indices = list(\n",
    "                [\n",
    "                    i\n",
    "                    for i in range(\n",
    "                        max(position - window_size, 0),\n",
    "                        min(position + window_size + 1, len(sentence)),\n",
    "                    )\n",
    "                    if i != position\n",
    "                ]\n",
    "            )\n",
    "            for index in context_indices:\n",
    "                context_word = sentence[index]\n",
    "                context_index = word2index[context_word]\n",
    "                skip_grams.append((center_index, context_index))\n",
    "\n",
    "    return skip_grams\n",
    "\n",
    "\n",
    "skip_grams = build_skipgrams(corpus, word2index, window_size=2)\n",
    "len(skip_grams)  # 5243836 pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigram_table(word_counts, power=0.75):\n",
    "    total_count = sum([count for count in word_counts.values()])\n",
    "    unigram_table = []\n",
    "    for word, count in word_counts.items():\n",
    "        score = (count / total_count) ** power\n",
    "        unigram_table.extend([word] * int(score * 1e6))\n",
    "    return unigram_table\n",
    "\n",
    "\n",
    "unigram_table = build_unigram_table(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, mode=\"softmax\"):\n",
    "        super(Skipgram, self).__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, center_words, next_words, all_vocabs):\n",
    "        center_embeds = self.embedding_v(center_words)\n",
    "        next_embeds = self.embedding_u(next_words)\n",
    "        all_embeds = self.embedding_u(all_vocabs)\n",
    "\n",
    "        scores = next_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        norm_scores = all_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        negative_log_likelihood = (-1) * (\n",
    "            torch.mean(\n",
    "                torch.log(torch.exp(scores) / torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return negative_log_likelihood"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
